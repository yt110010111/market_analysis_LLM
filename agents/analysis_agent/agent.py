# agents/analysis_agent/agent.py
import logging
from typing import Dict, Any, List
import requests
from report_generator import ReportGenerator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    åˆ†æä»£ç†ï¼šåˆ†ææœå°‹çµæœä¸¦å”èª¿å·¥ä½œæµ
    """
    
    def __init__(self):
        self.report_generator = ReportGenerator()
        self.web_scraping_url = "http://web_scraping_agent:8003"
        self.data_extraction_url = "http://data_extraction_agent:8004"
    
    def analyze_search_results(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææœå°‹çµæœä¸¦æ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•
        
        ç­–ç•¥ï¼š
        1. æª¢æŸ¥è³‡æ–™åº«ä¸­æ˜¯å¦å·²æœ‰è¶³å¤ è³‡æ–™
        2. å¦‚æœæœ‰ -> ç›´æ¥ç”Ÿæˆå ±å‘Š
        3. å¦‚æœæ²’æœ‰ -> åŸ·è¡Œçˆ¬èŸ²å’Œèƒå–æµç¨‹
        """
        query = search_results.get("query", "")
        results = search_results.get("results", [])
        
        logger.info(f"ğŸ” åˆ†ææœå°‹çµæœ: {query}")
        logger.info(f"   æ‰¾åˆ° {len(results)} å€‹æœå°‹çµæœ")
        
        # æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦
        coverage = self._check_database_coverage(query, results)
        
        if coverage["has_sufficient_data"]:
            logger.info("   âœ… è³‡æ–™åº«è³‡æ–™å……è¶³ï¼Œç›´æ¥ç”Ÿæˆå ±å‘Š")
            return {
                "action": "generate_report",
                "query": query,
                "reason": "è³‡æ–™åº«ä¸­å·²æœ‰è¶³å¤ çš„ç›¸é—œè³‡æ–™",
                "coverage": coverage,
                "search_results": results
            }
        else:
            logger.info("   âš ï¸ è³‡æ–™åº«è³‡æ–™ä¸è¶³ï¼Œéœ€è¦çˆ¬å–ç¶²é ")
            return {
                "action": "scrape_and_extract",
                "query": query,
                "reason": "éœ€è¦å¾ç¶²é çˆ¬å–æ›´å¤šè³‡æ–™",
                "coverage": coverage,
                "urls_to_scrape": [r.get("url") for r in results[:5]],  # é™åˆ¶ 5 å€‹
                "search_results": results
            }
    
    def _check_database_coverage(self, query: str, results: List[Dict]) -> Dict[str, Any]:
        """
        æª¢æŸ¥ Neo4j è³‡æ–™åº«ä¸­æ˜¯å¦æœ‰è¶³å¤ çš„ç›¸é—œè³‡æ–™
        """
        try:
            # ä½¿ç”¨ report_generator çš„ Neo4j æŸ¥è©¢æ–¹æ³•
            neo4j_data = self.report_generator._query_neo4j_knowledge(query)
            
            entity_count = neo4j_data.get("entity_count", 0)
            relationship_count = neo4j_data.get("relationship_count", 0)
            
            # åˆ¤æ–·æ¨™æº–ï¼šè‡³å°‘ 3 å€‹å¯¦é«”æˆ– 2 å€‹é—œä¿‚
            has_sufficient_data = entity_count >= 3 or relationship_count >= 2
            
            return {
                "has_sufficient_data": has_sufficient_data,
                "entity_count": entity_count,
                "relationship_count": relationship_count,
                "threshold": {"min_entities": 3, "min_relationships": 2}
            }
            
        except Exception as e:
            logger.warning(f"   âš ï¸ æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦å¤±æ•—: {e}")
            # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œé è¨­ç‚ºéœ€è¦çˆ¬å–
            return {
                "has_sufficient_data": False,
                "entity_count": 0,
                "relationship_count": 0,
                "error": str(e)
            }
    
    async def orchestrate_workflow(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ“š action åŸ·è¡Œç›¸æ‡‰çš„å·¥ä½œæµ
        
        è¿”å›æ ¼å¼ï¼š
        {
            "status": "success",
            "report": "...",  # é—œéµï¼å¿…é ˆåŒ…å«å ±å‘Šå…§å®¹
            "query": "...",
            "sources": {...}
        }
        """
        action = request.get("action")
        query = request.get("query")
        
        logger.info(f"ğŸ¬ é–‹å§‹åŸ·è¡Œå·¥ä½œæµ: {action}")
        
        try:
            if action == "generate_report":
                # ç›´æ¥ç”Ÿæˆå ±å‘Š
                search_results = request.get("search_results", [])
                report_data = self.report_generator.generate_comprehensive_report(
                    query=query,
                    search_results=search_results,
                    use_neo4j=True
                )
                
                logger.info(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
                return {
                    "status": "success",
                    "action": "generate_report",
                    "report": report_data["report"],  # é€™è£¡ï¼
                    "query": query,
                    "sources": report_data["sources"],
                    "generated_at": report_data["generated_at"]
                }
                
            elif action == "scrape_and_extract":
                # åŸ·è¡Œå®Œæ•´æµç¨‹ï¼šçˆ¬èŸ² -> èƒå– -> å„²å­˜ -> ç”Ÿæˆå ±å‘Š
                urls = request.get("urls_to_scrape", [])
                
                # æ­¥é©Ÿ 1: çˆ¬å–ç¶²é 
                logger.info(f"   ğŸ“¡ æ­¥é©Ÿ 1: çˆ¬å– {len(urls)} å€‹ç¶²é ")
                scraped_data = await self._scrape_urls(urls)
                
                # æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™
                logger.info(f"   ğŸ”¬ æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™")
                extracted_data = await self._extract_data(query, scraped_data)
                
                # æ­¥é©Ÿ 3: ç”Ÿæˆå ±å‘Šï¼ˆèƒå– agent å·²ç¶“å„²å­˜åˆ° Neo4jï¼‰
                logger.info(f"   ğŸ“ æ­¥é©Ÿ 3: ç”Ÿæˆæœ€çµ‚å ±å‘Š")
                search_results = request.get("search_results", [])
                report_data = self.report_generator.generate_comprehensive_report(
                    query=query,
                    search_results=search_results,
                    use_neo4j=True
                )
                
                logger.info(f"âœ… å®Œæ•´å·¥ä½œæµåŸ·è¡Œå®Œæˆ")
                return {
                    "status": "success",
                    "action": "scrape_and_extract",
                    "report": report_data["report"],  # é€™è£¡ï¼
                    "query": query,
                    "sources": report_data["sources"],
                    "workflow_steps": {
                        "scraped_urls": len(scraped_data),
                        "extracted_entities": extracted_data.get("entity_count", 0)
                    },
                    "generated_at": report_data["generated_at"]
                }
            
            else:
                raise ValueError(f"Unknown action: {action}")
                
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            return {
                "status": "error",
                "action": action,
                "query": query,
                "error": str(e),
                "report": f"# å ±å‘Šç”Ÿæˆå¤±æ•—\n\næŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            }
    
    async def _scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """å‘¼å« web_scraping_agent çˆ¬å–ç¶²é """
        try:
            response = requests.post(
                f"{self.web_scraping_url}/scrape",
                json={"urls": urls},
                timeout=60
            )
            response.raise_for_status()
            return response.json().get("results", [])
        except Exception as e:
            logger.error(f"   âŒ çˆ¬èŸ²å¤±æ•—: {e}")
            return []
    
    async def _extract_data(self, query: str, scraped_data: List[Dict]) -> Dict[str, Any]:
        """å‘¼å« data_extraction_agent èƒå–ä¸¦å„²å­˜è³‡æ–™"""
        try:
            response = requests.post(
                f"{self.data_extraction_url}/extract",
                json={
                    "query": query,
                    "documents": scraped_data
                },
                timeout=120
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"   âŒ è³‡æ–™èƒå–å¤±æ•—: {e}")
            return {"entity_count": 0, "error": str(e)}
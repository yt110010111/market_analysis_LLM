import os
import json
import logging
from typing import Dict, List, Any
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    åˆ†æä»£ç†ï¼šæ±ºå®šæ˜¯å¦éœ€è¦é¡å¤–è³‡æ–™æ”¶é›†æˆ–ç›´æ¥ç”Ÿæˆå ±å‘Š
    """
    
    def __init__(self):
        self.ollama_endpoint = os.getenv("OLLAMA_ENDPOINT", "http://ollama:11434")
        self.model_name = os.getenv("MODEL_NAME", "llama3.2:3b")
        self.neo4j_url = os.getenv("NEO4J_URL", "bolt://neo4j:7687")
        self.neo4j_user = os.getenv("NEO4J_USER", "neo4j")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD", "password123")
        self.web_scraping_agent_url = os.getenv("WEB_SCRAPING_AGENT_URL", "http://web_scraping_agent:8003")
        self.data_extraction_agent_url = os.getenv("DATA_EXTRACTION_AGENT_URL", "http://data_extraction_agent:8004")
        
    def analyze_search_results(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææœå°‹çµæœï¼Œæ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•
        
        Args:
            search_results: web_search_agent çš„çµæœ
            
        Returns:
            æ±ºç­–çµæœï¼ŒåŒ…å« action å’Œç›¸é—œè³‡æ–™
        """
        query = search_results.get("query", "")
        results = search_results.get("results", [])
        
        logger.info(f"ğŸ§  é–‹å§‹åˆ†ææœå°‹çµæœ: query='{query}', results_count={len(results)}")
        
        # æ­¥é©Ÿ 1: æª¢æŸ¥ Neo4j è³‡æ–™åº«ä¸­çš„ç›¸é—œè³‡æ–™
        db_coverage = self._check_database_coverage(query, results)
        
        # æ­¥é©Ÿ 2: æ ¹æ“šè³‡æ–™åº«è¦†è“‹åº¦æ±ºå®šè¡Œå‹•
        if db_coverage["sufficient"]:
            logger.info(f"âœ… åˆ¤æ–·: è³‡æ–™å……è¶³ï¼Œç›´æ¥ç”Ÿæˆå ±å‘Š")
            logger.info(f"   - è³‡æ–™åº«å¯¦é«”æ•¸: {db_coverage.get('db_entities_count', 0)}")
            logger.info(f"   - æœå°‹çµæœæ•¸: {len(results)}")
            return {
                "action": "generate_report",
                "query": query,
                "search_results": results,
                "db_data": db_coverage["data"]
            }
        else:
            logger.info(f"âŒ åˆ¤æ–·: è³‡æ–™ä¸è¶³ï¼Œéœ€è¦çˆ¬èŸ²æ”¶é›†")
            logger.info(f"   - è³‡æ–™åº«å¯¦é«”æ•¸: {db_coverage.get('db_entities_count', 0)}")
            logger.info(f"   - æœå°‹çµæœæ•¸: {len(results)}")
            logger.info(f"   - ç¼ºå°‘ä¸»é¡Œ: {db_coverage.get('missing_topics', [])}")
            
            urls_to_scrape = self._identify_scraping_targets(results)
            logger.info(f"   - è­˜åˆ¥åˆ° {len(urls_to_scrape)} å€‹çˆ¬å–ç›®æ¨™")
            
            return {
                "action": "scrape_and_extract",
                "query": query,
                "search_results": results,
                "missing_topics": db_coverage["missing_topics"],
                "urls_to_scrape": urls_to_scrape
            }
    
    def _check_database_coverage(self, query: str, search_results: List[Dict]) -> Dict[str, Any]:
        """
        æª¢æŸ¥ Neo4j è³‡æ–™åº«ä¸­æ˜¯å¦æœ‰è¶³å¤ çš„ç›¸é—œè³‡æ–™
        
        ç­–ç•¥ï¼š
        1. æŸ¥è©¢ Neo4j ä¸­ç›¸é—œçš„å¯¦é«”
        2. å¦‚æœè³‡æ–™åº«æœ‰è¶³å¤ å¯¦é«”ï¼Œä½¿ç”¨ Ollama åˆ¤æ–·å“è³ª
        3. å¦‚æœè³‡æ–™åº«ç‚ºç©ºæˆ–å¯¦é«”å¤ªå°‘ï¼Œç›´æ¥åˆ¤å®šä¸è¶³
        """
        logger.info(f"ğŸ“Š æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦...")
        
        try:
            # æ­¥é©Ÿ 1: æŸ¥è©¢ Neo4j ä¸­çš„ç›¸é—œå¯¦é«”
            db_entities = self._query_neo4j_entities(query)
            db_entities_count = len(db_entities)
            
            logger.info(f"   ğŸ“ è³‡æ–™åº«ä¸­æ‰¾åˆ° {db_entities_count} å€‹ç›¸é—œå¯¦é«”")
            
            # æ­¥é©Ÿ 2: åŸºæœ¬åˆ¤æ–· - å¦‚æœè³‡æ–™åº«å¹¾ä¹æ²’æœ‰è³‡æ–™ï¼Œç›´æ¥åˆ¤å®šä¸è¶³
            if db_entities_count < 6:
                logger.info(f"   âš ï¸ è³‡æ–™åº«å¯¦é«”ä¸è¶³ 3 å€‹ï¼Œåˆ¤å®šè³‡æ–™ä¸è¶³")
                return {
                    "sufficient": False,
                    "missing_topics": ["general", query],
                    "data": [],
                    "db_entities_count": db_entities_count,
                    "reason": "è³‡æ–™åº«å¯¦é«”æ•¸é‡ä¸è¶³"
                }
            
            # æ­¥é©Ÿ 3: å¦‚æœæœ‰ä¸€å®šæ•¸é‡çš„å¯¦é«”ï¼Œä½¿ç”¨ Ollama åˆ¤æ–·å“è³ª
            logger.info(f"   ğŸ¤– å‘¼å« Ollama è©•ä¼°è³‡æ–™å“è³ª...")
            
            prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡æ–™åˆ†æå°ˆå®¶ã€‚è«‹è©•ä¼°ä»¥ä¸‹è³‡æ–™æ˜¯å¦è¶³å¤ å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ä½¿ç”¨è€…å•é¡Œ: {query}

è³‡æ–™åº«ä¸­çš„å¯¦é«” ({db_entities_count} å€‹):
{json.dumps(db_entities[:10], ensure_ascii=False, indent=2)}

æœ€æ–°æœå°‹çµæœ ({len(search_results)} å€‹):
{json.dumps([{{
    'title': r.get('title', ''),
    'snippet': r.get('snippet', '')[:100]
}} for r in search_results[:3]], ensure_ascii=False, indent=2)}

è©•ä¼°æ¨™æº–:
1. è³‡æ–™åº«ä¸­çš„å¯¦é«”æ˜¯å¦èˆ‡å•é¡Œç›´æ¥ç›¸é—œï¼Ÿ
2. è³‡æ–™æ˜¯å¦è¶³å¤ æ–°ï¼Ÿ(è€ƒæ…®æœå°‹çµæœçš„æ—¥æœŸ)
3. è³‡æ–™æ˜¯å¦æ¶µè“‹å•é¡Œçš„å„å€‹ä¸»è¦æ–¹é¢ï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰:
{{
    "sufficient": true/false,
    "missing_topics": ["ç¼ºå°‘çš„ä¸»é¡Œ1", "ç¼ºå°‘çš„ä¸»é¡Œ2"],
    "reasoning": "åˆ¤æ–·ç†ç”±"
}}
"""
            
            response = self._call_ollama(prompt)
            logger.info(f"   ğŸ“ Ollama åŸå§‹å›æ‡‰: {response[:200]}...")
            
            # è§£æ LLM å›æ‡‰
            try:
                # æ¸…ç†å›æ‡‰ï¼ˆç§»é™¤å¯èƒ½çš„ markdownï¼‰
                clean_response = response.strip()
                if clean_response.startswith("```json"):
                    clean_response = clean_response[7:]
                if clean_response.startswith("```"):
                    clean_response = clean_response[3:]
                if clean_response.endswith("```"):
                    clean_response = clean_response[:-3]
                clean_response = clean_response.strip()
                
                decision = json.loads(clean_response)
                
                logger.info(f"   âœ… Ollama åˆ¤æ–·çµæœ:")
                logger.info(f"      - sufficient: {decision.get('sufficient', False)}")
                logger.info(f"      - reasoning: {decision.get('reasoning', 'N/A')}")
                logger.info(f"      - missing_topics: {decision.get('missing_topics', [])}")
                
            except json.JSONDecodeError as e:
                logger.error(f"   âŒ JSON è§£æå¤±æ•—: {e}")
                logger.error(f"   åŸå§‹å›æ‡‰: {response}")
                
                # è§£æå¤±æ•—ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼šå‡è¨­è³‡æ–™ä¸è¶³
                decision = {
                    "sufficient": False,
                    "missing_topics": [query],
                    "reasoning": "ç„¡æ³•è§£æ LLM å›æ‡‰ï¼Œæ¡ç”¨ä¿å®ˆç­–ç•¥"
                }
            
            return {
                "sufficient": decision.get("sufficient", False),
                "missing_topics": decision.get("missing_topics", []),
                "data": db_entities if decision.get("sufficient") else [],
                "db_entities_count": db_entities_count,
                "reasoning": decision.get("reasoning", ""),
                "llm_response": response[:500]  # ä¿ç•™å‰500å­—å…ƒä¾›é™¤éŒ¯
            }
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            
            # ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼Œå‡è¨­éœ€è¦æ›´å¤šè³‡æ–™
            return {
                "sufficient": False,
                "missing_topics": ["general"],
                "data": [],
                "db_entities_count": 0,
                "error": str(e)
            }
    
    def _query_neo4j_entities(self, query: str) -> List[Dict[str, Any]]:
        """
        æŸ¥è©¢ Neo4j ä¸­èˆ‡æŸ¥è©¢ç›¸é—œçš„å¯¦é«”
        """
        try:
            from neo4j import GraphDatabase
            
            driver = GraphDatabase.driver(
                self.neo4j_url,
                auth=(self.neo4j_user, self.neo4j_password)
            )
            
            entities = []
            
            with driver.session() as session:
                # æŸ¥è©¢èˆ‡ query ç›¸é—œçš„å¯¦é«”
                result = session.run("""
                    MATCH (q:Query)-[:FOUND]->(e:Entity)
                    WHERE q.text CONTAINS $query_keyword
                    RETURN e.name as name, e.type as type, 
                           e.description as description,
                           e.source_url as source_url
                    LIMIT 20
                """, query_keyword=query.split()[0])  # ä½¿ç”¨æŸ¥è©¢çš„ç¬¬ä¸€å€‹é—œéµå­—
                
                for record in result:
                    entities.append({
                        "name": record["name"],
                        "type": record["type"],
                        "description": record["description"],
                        "source_url": record["source_url"]
                    })
            
            driver.close()
            
            logger.info(f"   ğŸ” Neo4j æŸ¥è©¢å®Œæˆï¼Œæ‰¾åˆ° {len(entities)} å€‹å¯¦é«”")
            
            return entities
            
        except Exception as e:
            logger.warning(f"   âš ï¸ Neo4j æŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    def _identify_scraping_targets(self, search_results: List[Dict]) -> List[str]:
        """
        å¾æœå°‹çµæœä¸­è­˜åˆ¥éœ€è¦çˆ¬å–çš„ URL
        å„ªå…ˆé¸æ“‡çœŸå¯¦å¯è¨ªå•çš„ URL
        """
        urls = []
        for result in search_results[:10]:
            url = result.get("url")
            # éæ¿¾æ‰æ¨¡æ“¬ URL å’Œç„¡æ•ˆ URL
            if url and url.startswith("http") and "example.com" not in url:
                urls.append(url)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°çœŸå¯¦ URLï¼Œä½¿ç”¨é è¨­çš„é‡‘èè³‡æ–™ä¾†æº
        if not urls:
            query = search_results[0].get("title", "") if search_results else "stock"
            logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°çœŸå¯¦ URLï¼Œä½¿ç”¨é è¨­ä¾†æº")
            
            # æ ¹æ“šæŸ¥è©¢æ·»åŠ ç›¸é—œçš„é‡‘èç¶²ç«™
            if any(keyword in query.lower() for keyword in ["stock", "financial", "sofi", "investment"]):
                urls = [
                    "https://finance.yahoo.com",
                    "https://www.marketwatch.com",
                    "https://www.investing.com",
                    "https://seekingalpha.com",
                    "https://www.fool.com"
                ]
        
        logger.info(f"ğŸ“‹ è­˜åˆ¥åˆ° {len(urls)} å€‹çˆ¬å–ç›®æ¨™")
        if urls:
            for i, url in enumerate(urls[:3], 1):
                logger.info(f"   {i}. {url}")
        
        return urls[:5]
    
    def _call_ollama(self, prompt: str) -> str:
        """
        å‘¼å« Ollama API
        """
        try:
            logger.debug(f"ğŸ¤– å‘¼å« Ollama: {prompt[:100]}...")
            
            response = requests.post(
                f"{self.ollama_endpoint}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # é™ä½æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„ JSON
                        "num_predict": 500
                    }
                },
                timeout=100
            )
            response.raise_for_status()
            result = response.json().get("response", "")
            
            logger.debug(f"âœ… Ollama å›æ‡‰é•·åº¦: {len(result)} å­—å…ƒ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ollama å‘¼å«å¤±æ•—: {e}")
            raise
    
    async def orchestrate_workflow(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ“šåˆ†æçµæœç·¨æ’å¾ŒçºŒå·¥ä½œæµ
        """
        action = analysis_result.get("action")
        
        if action == "generate_report":
            return await self._generate_report(analysis_result)
        elif action == "scrape_and_extract":
            return await self._scrape_and_extract_workflow(analysis_result)
        else:
            raise ValueError(f"Unknown action: {action}")
    
    async def _generate_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç›´æ¥ç”Ÿæˆå ±å‘Šï¼ˆè³‡æ–™å……è¶³æ™‚ï¼‰
        """
        query = data.get("query")
        search_results = data.get("search_results", [])
        
        logger.info(f"ğŸ“ ç”Ÿæˆå ±å‘Š: {query}")
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™ï¼Œç”Ÿæˆä¸€ä»½è©³ç´°å ±å‘Šå›ç­”é€™å€‹å•é¡Œï¼š

å•é¡Œ: {query}

è³‡æ–™ä¾†æº:
{json.dumps(search_results[:5], ensure_ascii=False, indent=2)}

è«‹ç”Ÿæˆä¸€ä»½çµæ§‹åŒ–çš„å ±å‘Šï¼ŒåŒ…å«ï¼š
1. æ‘˜è¦ï¼ˆ2-3å¥è©±ï¼‰
2. ä¸»è¦ç™¼ç¾ï¼ˆ3-5é»ï¼‰
3. è©³ç´°åˆ†æï¼ˆ2-3æ®µï¼‰
4. çµè«–

å ±å‘Šï¼š
"""
        
        report = self._call_ollama(prompt)
        
        logger.info(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼Œé•·åº¦: {len(report)} å­—å…ƒ")
        
        return {
            "status": "completed",
            "action": "report_generated",
            "query": query,
            "report": report,
            "sources": [r.get("url") for r in search_results]
        }
    
    async def _scrape_and_extract_workflow(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸ·è¡Œçˆ¬èŸ²å’Œè³‡æ–™èƒå–å·¥ä½œæµ
        """
        urls = data.get("urls_to_scrape", [])
        query = data.get("query")
        
        logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬èŸ²å·¥ä½œæµ: {len(urls)} å€‹ URL")
        
        # æ­¥é©Ÿ 1: å‘¼å« web_scraping_agent
        try:
            logger.info(f"   ğŸ“¡ å‘¼å« web_scraping_agent...")
            scraping_response = requests.post(
                f"{self.web_scraping_agent_url}/scrape",
                json={
                    "urls": urls,
                    "query": query,
                    "dynamic_search": True
                },
                timeout=90
            )
            scraping_response.raise_for_status()
            scraped_data = scraping_response.json()
            logger.info(f"   âœ… çˆ¬èŸ²å®Œæˆ: {scraped_data.get('successful', 0)} æˆåŠŸ, {scraped_data.get('failed', 0)} å¤±æ•—")
        except Exception as e:
            logger.error(f"   âŒ web_scraping_agent éŒ¯èª¤: {e}")
            scraped_data = {"error": str(e), "results": []}
        
        # æ­¥é©Ÿ 2: å‘¼å« data_extraction_agent
        try:
            logger.info(f"   ğŸ“¡ å‘¼å« data_extraction_agent...")
            extraction_response = requests.post(
                f"{self.data_extraction_agent_url}/extract",
                json={"data": scraped_data, "query": query},
                timeout=90
            )
            extraction_response.raise_for_status()
            extracted_data = extraction_response.json()
            logger.info(f"   âœ… èƒå–å®Œæˆ: {len(extracted_data.get('entities', []))} å¯¦é«”, {len(extracted_data.get('relationships', []))} é—œä¿‚")
        except Exception as e:
            logger.error(f"   âŒ data_extraction_agent éŒ¯èª¤: {e}")
            extracted_data = {"error": str(e), "entities": [], "relationships": []}
        
        # æ­¥é©Ÿ 3: å„²å­˜åˆ° Neo4j
        logger.info(f"   ğŸ’¾ å„²å­˜åˆ° Neo4j...")
        neo4j_result = self._store_to_neo4j(extracted_data, query)
        
        # æ­¥é©Ÿ 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š
        logger.info(f"   ğŸ“ ç”Ÿæˆæœ€çµ‚å ±å‘Š...")
        report = self._generate_final_report(query, scraped_data, extracted_data)
        
        return {
            "status": "completed",
            "action": "data_collected_and_stored",
            "query": query,
            "scraped_urls": urls,
            "scraping_stats": {
                "successful": scraped_data.get("successful", 0),
                "failed": scraped_data.get("failed", 0)
            },
            "extraction_stats": {
                "entities": len(extracted_data.get("entities", [])),
                "relationships": len(extracted_data.get("relationships", []))
            },
            "neo4j_stored": neo4j_result.get("success", False),
            "report": report
        }
    
    def _store_to_neo4j(self, extracted_data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        å°‡æå–çš„å¯¦é«”å’Œé—œä¿‚å„²å­˜åˆ° Neo4j
        """
        try:
            from neo4j import GraphDatabase
            
            driver = GraphDatabase.driver(
                self.neo4j_url,
                auth=(self.neo4j_user, self.neo4j_password)
            )
            
            with driver.session() as session:
                # å‰µå»ºæŸ¥è©¢ç¯€é»
                session.run(
                    "MERGE (q:Query {text: $query_text}) SET q.timestamp = datetime() RETURN q",
                    query_text=query
                )
                
                # å‰µå»ºå¯¦é«”ç¯€é»
                entities = extracted_data.get("entities", [])
                for entity in entities:
                    session.run(
                        """
                        MERGE (e:Entity {name: $name})
                        SET e.type = $type,
                            e.description = $description,
                            e.source_url = $source_url,
                            e.updated_at = datetime()
                        WITH e
                        MATCH (q:Query {text: $query_text})
                        MERGE (q)-[:FOUND]->(e)
                        """,
                        name=entity.get("name"),
                        type=entity.get("type"),
                        description=entity.get("description"),
                        source_url=entity.get("source_url"),
                        query_text=query
                    )
                
                # å‰µå»ºé—œä¿‚
                relationships = extracted_data.get("relationships", [])
                for rel in relationships:
                    session.run(
                        """
                        MATCH (source:Entity {name: $source})
                        MATCH (target:Entity {name: $target})
                        MERGE (source)-[r:RELATES_TO {type: $relation}]->(target)
                        SET r.description = $description,
                            r.updated_at = datetime()
                        """,
                        source=rel.get("source"),
                        target=rel.get("target"),
                        relation=rel.get("relation"),
                        description=rel.get("description")
                    )
            
            driver.close()
            
            logger.info(f"   âœ… Neo4j å„²å­˜æˆåŠŸ: {len(entities)} å¯¦é«”, {len(relationships)} é—œä¿‚")
            
            return {
                "success": True,
                "entities_stored": len(entities),
                "relationships_stored": len(relationships)
            }
            
        except Exception as e:
            logger.error(f"   âŒ Neo4j å„²å­˜å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _generate_final_report(self, query: str, scraped_data: Dict, extracted_data: Dict) -> str:
        """
        åŸºæ–¼çˆ¬å–å’Œèƒå–çš„è³‡æ–™ç”Ÿæˆæœ€çµ‚å ±å‘Š
        """
        summary = extracted_data.get("overall_summary", "")
        entities = extracted_data.get("entities", [])
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡è¨Šï¼Œç”Ÿæˆä¸€ä»½é—œæ–¼ã€Œ{query}ã€çš„è©³ç´°ç ”ç©¶å ±å‘Šã€‚

æ•´é«”æ‘˜è¦:
{summary}

ç™¼ç¾çš„é—œéµå¯¦é«”:
{', '.join([e.get('name', '') for e in entities[:10]])}

è«‹ç”Ÿæˆä¸€ä»½çµæ§‹åŒ–å ±å‘Šï¼ŒåŒ…å«ï¼š
1. åŸ·è¡Œæ‘˜è¦ï¼ˆ2-3 æ®µï¼‰
2. ä¸»è¦ç™¼ç¾
3. è©³ç´°åˆ†æ
4. çµè«–å’Œå»ºè­°

å ±å‘Šï¼š
"""
        
        try:
            report = self._call_ollama(prompt)
            return report
        except Exception as e:
            logger.error(f"Error generating report: {e}")
            return f"åŸºæ–¼æ”¶é›†çš„è³‡æ–™ï¼Œé—œæ–¼{query}çš„ä¸»è¦ç™¼ç¾å¦‚ä¸‹ï¼š\n\n{summary}"
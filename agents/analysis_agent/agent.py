# agents/analysis_agent/agent.py
import logging
from typing import Dict, Any, List
import requests
from report_generator import ReportGenerator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    åˆ†æä»£ç†ï¼šåˆ†ææœå°‹çµæœä¸¦å”èª¿å·¥ä½œæµ
    """
    
    def __init__(self):
        self.report_generator = ReportGenerator()
        self.web_scraping_url = "http://web_scraping_agent:8003"
        self.data_extraction_url = "http://data_extraction_agent:8004"
    
    def analyze_search_results(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææœå°‹çµæœä¸¦æ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•
        
        ç­–ç•¥ï¼š
        1. æª¢æŸ¥è³‡æ–™åº«ä¸­æ˜¯å¦å·²æœ‰è¶³å¤ è³‡æ–™
        2. å¦‚æœæœ‰ -> ç›´æ¥ç”Ÿæˆå ±å‘Š
        3. å¦‚æœæ²’æœ‰ -> åŸ·è¡Œçˆ¬èŸ²å’Œèƒå–æµç¨‹
        """
        query = search_results.get("query", "")
        results = search_results.get("results", [])
        
        logger.info(f"ğŸ” åˆ†ææœå°‹çµæœ: {query}")
        logger.info(f"   æ‰¾åˆ° {len(results)} å€‹æœå°‹çµæœ")
        
        # æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦
        coverage = self._check_database_coverage(query, results)
        
        if coverage["has_sufficient_data"]:
            logger.info("   âœ… è³‡æ–™åº«è³‡æ–™å……è¶³ï¼Œç›´æ¥ç”Ÿæˆå ±å‘Š")
            return {
                "action": "generate_report",
                "query": query,
                "reason": "è³‡æ–™åº«ä¸­å·²æœ‰è¶³å¤ çš„ç›¸é—œè³‡æ–™",
                "coverage": coverage,
                "search_results": results
            }
        else:
            logger.info("   âš ï¸ è³‡æ–™åº«è³‡æ–™ä¸è¶³ï¼Œéœ€è¦çˆ¬å–ç¶²é ")
            return {
                "action": "scrape_and_extract",
                "query": query,
                "reason": "éœ€è¦å¾ç¶²é çˆ¬å–æ›´å¤šè³‡æ–™",
                "coverage": coverage,
                "urls_to_scrape": [r.get("url") for r in results[:5]],  # é™åˆ¶ 5 å€‹
                "search_results": results
            }
    
    def _check_database_coverage(self, query: str, results: List[Dict]) -> Dict[str, Any]:
        """
        æª¢æŸ¥ Neo4j è³‡æ–™åº«ä¸­æ˜¯å¦æœ‰è¶³å¤ çš„ç›¸é—œè³‡æ–™
        """
        try:
            # ä½¿ç”¨ report_generator çš„ Neo4j æŸ¥è©¢æ–¹æ³•
            neo4j_data = self.report_generator._query_neo4j_knowledge(query)
            
            entity_count = neo4j_data.get("entity_count", 0)
            relationship_count = neo4j_data.get("relationship_count", 0)
            
            # åˆ¤æ–·æ¨™æº–ï¼šè‡³å°‘ 3 å€‹å¯¦é«”æˆ– 2 å€‹é—œä¿‚
            has_sufficient_data = entity_count >= 3 or relationship_count >= 2
            
            return {
                "has_sufficient_data": has_sufficient_data,
                "entity_count": entity_count,
                "relationship_count": relationship_count,
                "threshold": {"min_entities": 3, "min_relationships": 2}
            }
            
        except Exception as e:
            logger.warning(f"   âš ï¸ æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦å¤±æ•—: {e}")
            # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œé è¨­ç‚ºéœ€è¦çˆ¬å–
            return {
                "has_sufficient_data": False,
                "entity_count": 0,
                "relationship_count": 0,
                "error": str(e)
            }
    
    async def orchestrate_workflow(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ“š action åŸ·è¡Œç›¸æ‡‰çš„å·¥ä½œæµ
        
        è¿”å›æ ¼å¼ï¼š
        {
            "status": "success",
            "report": "...",
            "query": "...",
            "sources": {...}
        }
        """
        action = request.get("action")
        query = request.get("query")
        
        logger.info(f"ğŸ¬ é–‹å§‹åŸ·è¡Œå·¥ä½œæµ: {action}")
        
        try:
            if action == "generate_report":
                # ç›´æ¥ç”Ÿæˆå ±å‘Š
                search_results = request.get("search_results", [])
                report_data = self.report_generator.generate_comprehensive_report(
                    query=query,
                    search_results=search_results,
                    use_neo4j=True
                )
                
                logger.info(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
                return {
                    "status": "success",
                    "action": "generate_report",
                    "report": report_data["report"],
                    "query": query,
                    "sources": report_data["sources"],
                    "generated_at": report_data["generated_at"]
                }
                
            elif action == "scrape_and_extract":
                # åŸ·è¡Œå®Œæ•´æµç¨‹ï¼šTavily æœå°‹ + çˆ¬èŸ² -> èƒå– -> å„²å­˜ -> ç”Ÿæˆå ±å‘Š
                
                # æ­¥é©Ÿ 1: ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é 
                logger.info(f"   ğŸ” æ­¥é©Ÿ 1: ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é ")
                scraped_data = await self._search_and_scrape(query)
                
                if not scraped_data.get("results"):
                    logger.warning("   âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç¶²é è³‡æ–™")
                    report_data = self.report_generator.generate_comprehensive_report(
                        query=query,
                        search_results=[],
                        use_neo4j=True
                    )
                    return {
                        "status": "success",
                        "action": "scrape_and_extract",
                        "report": report_data["report"],
                        "query": query,
                        "sources": report_data["sources"],
                        "workflow_steps": {
                            "scraped_urls": 0,
                            "extracted_entities": 0,
                            "note": "æœªæ‰¾åˆ°æ–°è³‡æ–™ï¼Œä½¿ç”¨ç¾æœ‰è³‡æ–™åº«ç”Ÿæˆå ±å‘Š"
                        },
                        "generated_at": report_data["generated_at"]
                    }
                
                # æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™ï¼ˆèƒå– agent æœƒè‡ªå‹•å­˜å…¥ Neo4jï¼‰
                logger.info(f"   ğŸ”¬ æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™ä¸¦å­˜å…¥ Neo4j")
                extracted_data = await self._extract_data(query, scraped_data)
                
                # âœ… é—œéµä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨èƒå–çµæœï¼Œä¸å†æŸ¥è©¢ Neo4j
                entities = extracted_data.get("entities", [])
                relationships = extracted_data.get("relationships", [])
                
                logger.info(f"   ğŸ“ æ­¥é©Ÿ 3: ä½¿ç”¨èƒå–çµæœç”Ÿæˆå ±å‘Š")
                logger.info(f"   ğŸ“Š ä½¿ç”¨ {len(entities)} å€‹å¯¦é«”å’Œ {len(relationships)} å€‹é—œä¿‚")
                
                # ç›´æ¥å‚³éèƒå–çš„å¯¦é«”å’Œé—œä¿‚çµ¦å ±å‘Šç”Ÿæˆå™¨
                report_data = self.report_generator.generate_report_from_extraction(
                    query=query,
                    entities=entities,
                    relationships=relationships,
                    search_results=scraped_data.get("results", [])
                )
                
                logger.info(f"âœ… å®Œæ•´å·¥ä½œæµåŸ·è¡Œå®Œæˆ")
                return {
                    "status": "success",
                    "action": "scrape_and_extract",
                    "report": report_data["report"],
                    "query": query,
                    "sources": report_data["sources"],
                    "workflow_steps": {
                        "scraped_urls": len(scraped_data.get("results", [])),
                        "extracted_entities": len(entities),
                        "extracted_relationships": len(relationships)
                    },
                    "generated_at": report_data["generated_at"]
                }
            
            else:
                raise ValueError(f"Unknown action: {action}")
                
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            return {
                "status": "error",
                "action": action,
                "query": query,
                "error": str(e),
                "report": f"# å ±å‘Šç”Ÿæˆå¤±æ•—\n\næŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            }
    
    async def _search_and_scrape(self, query: str) -> Dict[str, Any]:
        """
        ğŸ†• ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é ï¼ˆä¸€æ¬¡å®Œæˆï¼‰
        
        é€™å€‹æ–¹æ³•æœƒï¼š
        1. èª¿ç”¨ web_scraping_agent
        2. å‚³å…¥ query å’Œ dynamic_search=True
        3. web_scraping_agent æœƒè‡ªå‹•ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–
        
        è¿”å›æ ¼å¼ï¼š
        {
            "query": str,
            "total_urls": int,
            "successful": int,
            "failed": int,
            "results": [...]
        }
        """
        try:
            response = requests.post(
                f"{self.web_scraping_url}/scrape",
                json={
                    "urls": [],  # ç©ºåˆ—è¡¨ï¼Œè®“å®ƒè‡ªå·±ç”¨ Tavily æœå°‹
                    "query": query,
                    "dynamic_search": True  # å•Ÿç”¨ Tavily
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            
            logger.info(f"   âœ… æœå°‹ä¸¦çˆ¬å–å®Œæˆ: {result.get('successful', 0)} å€‹æˆåŠŸ")
            return result
            
        except Exception as e:
            logger.error(f"   âŒ æœå°‹ä¸¦çˆ¬å–å¤±æ•—: {e}")
            return {"results": []}
    
    async def _extract_data(self, query: str, scraped_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‘¼å« data_extraction_agent èƒå–ä¸¦å„²å­˜è³‡æ–™
        """
        try:
            response = requests.post(
                f"{self.data_extraction_url}/extract",
                json={
                    "data": scraped_data,
                    "query": query
                },
                timeout=120
            )
            response.raise_for_status()
            result = response.json()
            
            # è¨˜éŒ„æˆåŠŸè¨Šæ¯
            stats = result.get("statistics", {})
            entity_count = stats.get("total_entities", 0)
            rel_count = stats.get("total_relationships", 0)
            logger.info(f"   âœ… èƒå–æˆåŠŸ: {entity_count} å€‹å¯¦é«”, {rel_count} å€‹é—œä¿‚")
            
            # âœ… æª¢æŸ¥ Neo4j å­˜å„²ç‹€æ…‹
            storage_status = result.get("neo4j_storage", {})
            if storage_status.get("status") == "error":
                logger.warning(f"   âš ï¸ Neo4j å­˜å„²å¤±æ•—: {storage_status.get('error')}")
            elif storage_status.get("status") == "success":
                logger.info(f"   âœ… Neo4j å­˜å„²æˆåŠŸ: {storage_status.get('entities_stored')} å¯¦é«”, {storage_status.get('relationships_stored')} é—œä¿‚")
            
            return result
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"   âŒ è³‡æ–™èƒå–å¤±æ•—: {e.response.status_code} - {e.response.text}")
            # âœ… è¿”å›ç©ºçµæœè€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸
            return {
                "entities": [],
                "relationships": [],
                "statistics": {"total_entities": 0, "total_relationships": 0},
                "error": str(e)
            }
        except Exception as e:
            logger.error(f"   âŒ è³‡æ–™èƒå–å¤±æ•—: {e}")
            # âœ… è¿”å›ç©ºçµæœè€Œä¸æ˜¯æ‹‹å‡ºç•°å¸¸
            return {
                "entities": [],
                "relationships": [],
                "statistics": {"total_entities": 0, "total_relationships": 0},
                "error": str(e)
            }
import logging
from typing import Dict, Any, List
import requests
import json
from report_generator import ReportGenerator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AnalysisAgent:
    """
    åˆ†æä»£ç†ï¼šä½¿ç”¨ LLM åˆ¤æ–·è³‡æ–™å……è¶³åº¦ä¸¦å”èª¿å·¥ä½œæµ
    """
    
    def __init__(self):
        self.report_generator = ReportGenerator()
        self.web_scraping_url = "http://web_scraping_agent:8003"
        self.data_extraction_url = "http://data_extraction_agent:8004"
        self.ollama_endpoint = "http://ollama:11434"
        self.model_name = "llama3.2:3b"
        self.max_iterations = 3  # æœ€å¤šè¿­ä»£ 3 æ¬¡
    
    def _query_ollama(self, prompt: str, temperature: float = 0.3) -> str:
        """
        å‘¼å« Ollama API
        """
        try:
            response = requests.post(
                f"{self.ollama_endpoint}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "temperature": temperature,
                    "stream": False
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            return result.get("response", "").strip()
        except Exception as e:
            logger.error(f"âŒ Ollama å‘¼å«å¤±æ•—: {e}")
            raise
    
    def _check_data_sufficiency_with_llm(
        self, 
        query: str, 
        entities: List[Dict], 
        relationships: List[Dict],
        iteration: int = 0
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM åˆ¤æ–·è³‡æ–™æ˜¯å¦å……è¶³ä»¥æ’°å¯«å ±å‘Š
        
        Returns:
            {
                "is_sufficient": bool,
                "reason": str,
                "missing_aspects": List[str],
                "confidence": float
            }
        """
        # æ§‹å»ºå¯¦é«”æ‘˜è¦
        entity_summary = self._summarize_entities(entities)
        relationship_summary = self._summarize_relationships(relationships)
        
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¸‚å ´åˆ†æåŠ©ç†ã€‚è«‹åˆ¤æ–·ä»¥ä¸‹è³‡æ–™æ˜¯å¦è¶³ä»¥æ’°å¯«ä¸€ä»½å®Œæ•´çš„å¸‚å ´åˆ†æå ±å‘Šã€‚

æŸ¥è©¢ä¸»é¡Œ: {query}

ç›®å‰æ”¶é›†çš„è³‡æ–™:
- å¯¦é«”æ•¸é‡: {len(entities)}
- é—œä¿‚æ•¸é‡: {len(relationships)}
- è¿­ä»£æ¬¡æ•¸: {iteration + 1}/{self.max_iterations}

å¯¦é«”æ‘˜è¦:
{entity_summary}

é—œä¿‚æ‘˜è¦:
{relationship_summary}

è«‹è©•ä¼°:
1. è³‡æ–™æ˜¯å¦æ¶µè“‹ä¸»é¡Œçš„æ ¸å¿ƒé¢å‘ï¼Ÿ
2. æ˜¯å¦æœ‰è¶³å¤ çš„ç´°ç¯€æ”¯æ’åˆ†æï¼Ÿ
3. é—œä¿‚æ˜¯å¦è¶³ä»¥å»ºç«‹å› æœæˆ–é—œè¯åˆ†æï¼Ÿ
4. é‚„ç¼ºå°‘å“ªäº›é‡è¦è³‡è¨Šï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼ˆåªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰:
{{
    "is_sufficient": true/false,
    "confidence": 0.0-1.0,
    "reason": "ç°¡çŸ­èªªæ˜",
    "missing_aspects": ["ç¼ºå°‘çš„é¢å‘1", "ç¼ºå°‘çš„é¢å‘2"],
    "coverage_score": 0-100
}}"""

        try:
            llm_response = self._query_ollama(prompt, temperature=0.3)
            
            # å˜—è©¦è§£æ JSON
            # ç§»é™¤å¯èƒ½çš„ markdown æ¨™è¨˜
            llm_response = llm_response.replace("```json", "").replace("```", "").strip()
            
            result = json.loads(llm_response)
            
            logger.info(f"ğŸ¤– LLM åˆ¤æ–·çµæœ:")

            logger.info(f"   åŸå› : {result.get('reason', 'N/A')}")
            if result.get('missing_aspects'):
                logger.info(f"   ç¼ºå°‘é¢å‘: {', '.join(result.get('missing_aspects', []))}")
            
            return result
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ LLM å›æ‡‰è§£æå¤±æ•—: {e}")
            logger.warning(f"   åŸå§‹å›æ‡‰: {llm_response[:200]}")
            
            # é™ç´šè™•ç†ï¼šä½¿ç”¨ç°¡å–®è¦å‰‡
            return self._fallback_sufficiency_check(entities, relationships)
        except Exception as e:
            logger.error(f"âŒ LLM åˆ¤æ–·å¤±æ•—: {e}")
            return self._fallback_sufficiency_check(entities, relationships)
    
    def _fallback_sufficiency_check(
        self, 
        entities: List[Dict], 
        relationships: List[Dict]
    ) -> Dict[str, Any]:
        """
        é™ç´šæ–¹æ¡ˆï¼šä½¿ç”¨ç°¡å–®è¦å‰‡åˆ¤æ–·
        """
        entity_count = len(entities)
        rel_count = len(relationships)
        
        # ç°¡å–®è¦å‰‡ï¼šè‡³å°‘ 5 å€‹å¯¦é«”å’Œ 3 å€‹é—œä¿‚
        is_sufficient = entity_count >= 5 and rel_count >= 3
        coverage_score = min(100, (entity_count * 10 + rel_count * 15))
        
        return {
            "is_sufficient": is_sufficient,
            "confidence": 0.6,
            "reason": f"åŸºæ–¼è¦å‰‡åˆ¤æ–·ï¼š{entity_count} å¯¦é«”, {rel_count} é—œä¿‚",
            "missing_aspects": ["éœ€è¦æ›´å¤šè³‡æ–™"] if not is_sufficient else [],
            "coverage_score": coverage_score
        }
    
    def _summarize_entities(self, entities: List[Dict]) -> str:
        """
        æ‘˜è¦å¯¦é«”è³‡è¨Š
        """
        if not entities:
            return "ç„¡å¯¦é«”è³‡æ–™"
        
        # çµ±è¨ˆå¯¦é«”é¡å‹
        type_counts = {}
        for entity in entities[:20]:  # åªçœ‹å‰ 20 å€‹
            entity_type = entity.get("type", "Unknown")
            type_counts[entity_type] = type_counts.get(entity_type, 0) + 1
        
        summary_lines = [f"- {type_}: {count} å€‹" for type_, count in type_counts.items()]
        
        # åˆ—å‡ºä¸€äº›å¯¦é«”åç¨±
        sample_names = [e.get("name", "N/A") for e in entities[:5]]
        summary_lines.append(f"ç¯„ä¾‹: {', '.join(sample_names)}")
        
        return "\n".join(summary_lines)
    
    def _summarize_relationships(self, relationships: List[Dict]) -> str:
        """
        æ‘˜è¦é—œä¿‚è³‡è¨Š
        """
        if not relationships:
            return "ç„¡é—œä¿‚è³‡æ–™"
        
        # çµ±è¨ˆé—œä¿‚é¡å‹
        type_counts = {}
        for rel in relationships[:20]:  # åªçœ‹å‰ 20 å€‹
            rel_type = rel.get("type", "Unknown")
            type_counts[rel_type] = type_counts.get(rel_type, 0) + 1
        
        summary_lines = [f"- {type_}: {count} å€‹" for type_, count in type_counts.items()]
        
        return "\n".join(summary_lines)
    
    def _generate_search_queries(self, query: str, missing_aspects: List[str]) -> List[str]:
        """
        æ ¹æ“šç¼ºå°‘çš„é¢å‘ç”Ÿæˆæ–°çš„æœå°‹æŸ¥è©¢
        """
        if not missing_aspects:
            return [query]
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡è¨Šï¼Œç”Ÿæˆ 2-3 å€‹æ›´å…·é«”çš„æœå°‹æŸ¥è©¢ä¾†è£œå……ç¼ºå°‘çš„è³‡è¨Šã€‚

åŸå§‹æŸ¥è©¢: {query}
ç¼ºå°‘çš„é¢å‘: {', '.join(missing_aspects)}

è«‹ç”Ÿæˆèƒ½å¤ æ‰¾åˆ°é€™äº›ç¼ºå°‘è³‡è¨Šçš„æœå°‹æŸ¥è©¢ã€‚
æ ¼å¼: æ¯è¡Œä¸€å€‹æŸ¥è©¢ï¼Œä¸è¦ç·¨è™Ÿæˆ–å…¶ä»–æ¨™è¨˜ã€‚
ç¯„ä¾‹:
å°ç£ AI ç”¢æ¥­ ä¾›æ‡‰éˆ
å°ç£ AI æ™¶ç‰‡ å¸‚å ´è¦æ¨¡
å°ç£ AI æ–°å‰µå…¬å¸"""

        try:
            llm_response = self._query_ollama(prompt, temperature=0.7)
            queries = [q.strip() for q in llm_response.split("\n") if q.strip()]
            queries = queries[:3]  # æœ€å¤š 3 å€‹
            
            logger.info(f"ğŸ” ç”Ÿæˆæ–°æœå°‹æŸ¥è©¢: {queries}")
            return queries
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆæœå°‹æŸ¥è©¢å¤±æ•—: {e}")
            return [query]
    
    async def orchestrate_workflow(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿­ä»£å¼å·¥ä½œæµï¼šä¸æ–·æœå°‹ç›´åˆ°è³‡æ–™å……è¶³
        
        æµç¨‹:
        1. æª¢æŸ¥ç¾æœ‰è³‡æ–™
        2. LLM åˆ¤æ–·æ˜¯å¦å……è¶³
        3. å¦‚æœä¸è¶³ â†’ æœå°‹ + çˆ¬å– + èƒå– â†’ å›åˆ°æ­¥é©Ÿ 2
        4. å¦‚æœå……è¶³æˆ–é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ â†’ ç”Ÿæˆå ±å‘Š
        """
        query = request.get("query")
        action = request.get("action")
        
        logger.info(f"ğŸ¬ é–‹å§‹åŸ·è¡Œè¿­ä»£å¼å·¥ä½œæµ: {query}")
        
        all_scraped_results = []
        iteration = 0
        
        try:
            while iteration < self.max_iterations:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“ è¿­ä»£ {iteration + 1}/{self.max_iterations}")
                logger.info(f"{'='*60}")
                
                # ============ æ­¥é©Ÿ 1: æŸ¥è©¢ç¾æœ‰è³‡æ–™ ============
                logger.info(f"ğŸ” æ­¥é©Ÿ 1: æŸ¥è©¢ Neo4j ç¾æœ‰è³‡æ–™")
                neo4j_data = self.report_generator._query_neo4j_knowledge(query)
                entities = neo4j_data.get("entities", [])
                relationships = neo4j_data.get("relationships", [])
                
                logger.info(f"ğŸ“Š ç•¶å‰è³‡æ–™: {len(entities)} å¯¦é«”, {len(relationships)} é—œä¿‚")
                
                # ============ æ­¥é©Ÿ 2: LLM åˆ¤æ–·å……è¶³åº¦ ============
                logger.info(f"ğŸ¤– æ­¥é©Ÿ 2: LLM åˆ¤æ–·è³‡æ–™å……è¶³åº¦")
                sufficiency = self._check_data_sufficiency_with_llm(
                    query, entities, relationships, iteration
                )
                
                # ============ æ­¥é©Ÿ 3: æ±ºå®šæ˜¯å¦ç¹¼çºŒ ============
                if sufficiency.get("is_sufficient", False):
                    logger.info(f"âœ… LLM åˆ¤æ–·è³‡æ–™å……è¶³ï¼Œé–‹å§‹ç”Ÿæˆå ±å‘Š")
                    break
                
                if iteration >= self.max_iterations - 1:
                    logger.info(f"âš ï¸ å·²é”æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œå¼·åˆ¶ç”Ÿæˆå ±å‘Š")
                    break
                
                # ============ æ­¥é©Ÿ 4: ç”Ÿæˆæ–°æœå°‹æŸ¥è©¢ ============
                logger.info(f"ğŸ“ æ­¥é©Ÿ 3: ç”Ÿæˆè£œå……æœå°‹æŸ¥è©¢")
                missing_aspects = sufficiency.get("missing_aspects", [])
                search_queries = self._generate_search_queries(query, missing_aspects)
                
                # ============ æ­¥é©Ÿ 5: æœå°‹ + çˆ¬å– ============
                logger.info(f"ğŸ” æ­¥é©Ÿ 4: åŸ·è¡Œæœå°‹å’Œçˆ¬å–")
                for search_query in search_queries:
                    logger.info(f"   æœå°‹: {search_query}")
                    scraped_data = await self._search_and_scrape(search_query)
                    
                    if scraped_data.get("results"):
                        all_scraped_results.extend(scraped_data.get("results", []))
                        
                        # ============ æ­¥é©Ÿ 6: èƒå–ä¸¦å­˜å…¥ Neo4j ============
                        logger.info(f"   ğŸ”¬ èƒå–è³‡æ–™ä¸¦å­˜å…¥ Neo4j")
                        await self._extract_data(query, scraped_data)
                
                iteration += 1
            
            # ============ æœ€çµ‚æ­¥é©Ÿ: ç”Ÿæˆå ±å‘Š ============
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“ æœ€çµ‚æ­¥é©Ÿ: ç”Ÿæˆå ±å‘Š")
            logger.info(f"{'='*60}")
            
            # é‡æ–°æŸ¥è©¢æœ€çµ‚è³‡æ–™
            final_neo4j_data = self.report_generator._query_neo4j_knowledge(query)
            final_entities = final_neo4j_data.get("entities", [])
            final_relationships = final_neo4j_data.get("relationships", [])
            
            logger.info(f"ğŸ“Š æœ€çµ‚è³‡æ–™: {len(final_entities)} å¯¦é«”, {len(final_relationships)} é—œä¿‚")
            
            report_data = self.report_generator.generate_report_from_extraction(
                query=query,
                entities=final_entities,
                relationships=final_relationships,
                search_results=all_scraped_results
            )
            
            logger.info(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
            
            return {
                "status": "success",
                "action": action,
                "report": report_data["report"],
                "query": query,
                "sources": report_data["sources"],
                "workflow_steps": {
                    "iterations": iteration + 1,
                    "total_scraped_urls": len(all_scraped_results),
                    "final_entities": len(final_entities),
                    "final_relationships": len(final_relationships),
                    "sufficiency_score": sufficiency.get("coverage_score", 0)
                },
                "generated_at": report_data["generated_at"]
            }
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            return {
                "status": "error",
                "action": action,
                "query": query,
                "error": str(e),
                "report": f"# å ±å‘Šç”Ÿæˆå¤±æ•—\n\næŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            }
    
    async def _search_and_scrape(self, query: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é 
        """
        try:
            response = requests.post(
                f"{self.web_scraping_url}/scrape",
                json={
                    "urls": [],
                    "query": query,
                    "dynamic_search": True
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            
            logger.info(f"   âœ… çˆ¬å–å®Œæˆ: {result.get('successful', 0)} å€‹æˆåŠŸ")
            return result
            
        except Exception as e:
            logger.error(f"   âŒ æœå°‹çˆ¬å–å¤±æ•—: {e}")
            return {"results": []}
    
    async def _extract_data(self, query: str, scraped_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‘¼å« data_extraction_agent èƒå–ä¸¦å„²å­˜è³‡æ–™
        """
        try:
            response = requests.post(
                f"{self.data_extraction_url}/extract",
                json={
                    "data": scraped_data,
                    "query": query
                },
                timeout=120
            )
            response.raise_for_status()
            result = response.json()
            
            stats = result.get("statistics", {})
            entity_count = stats.get("total_entities", 0)
            rel_count = stats.get("total_relationships", 0)
            logger.info(f"   âœ… èƒå–æˆåŠŸ: {entity_count} å€‹å¯¦é«”, {rel_count} å€‹é—œä¿‚")
            
            return result
            
        except Exception as e:
            logger.error(f"   âŒ è³‡æ–™èƒå–å¤±æ•—: {e}")
            return {
                "entities": [],
                "relationships": [],
                "statistics": {"total_entities": 0, "total_relationships": 0},
                "error": str(e)
            }
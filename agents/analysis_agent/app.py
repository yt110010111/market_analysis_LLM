# agents/analysis_agent/app.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import logging
from agent import AnalysisAgent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Analysis Agent API")

# â­ æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

agent = AnalysisAgent()


class AnalyzeRequest(BaseModel):
    """å‰ç«¯çµ±ä¸€å…¥å£çš„è«‹æ±‚æ ¼å¼"""
    query: str


class AnalyzeResponse(BaseModel):
    """çµ±ä¸€å›æ‡‰æ ¼å¼"""
    status: str
    query: str
    action: str
    report: str
    sources: Dict[str, Any]
    workflow_steps: Optional[Dict[str, Any]] = None
    generated_at: str


@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "analysis_agent",
        "version": "0.3.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "analyze": "/analyze (POST) - çµ±ä¸€å…¥å£",
            "orchestrate": "/orchestrate (POST) - å…§éƒ¨ä½¿ç”¨",
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {"status": "healthy", "service": "analysis_agent"}


@app.post("/analyze")
async def analyze_query(request: AnalyzeRequest):
    """
    ğŸ¯ çµ±ä¸€å…¥å£ï¼šå‰ç«¯åªéœ€è¦èª¿ç”¨é€™å€‹ç«¯é»
    
    å„ªåŒ–å¾Œçš„æµç¨‹ï¼š
    1. ç›´æ¥æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦ âœ…
    2. å¦‚æœè³‡æ–™å……è¶³ â†’ ç«‹å³ç”Ÿæˆå ±å‘Š âœ…
    3. å¦‚æœè³‡æ–™ä¸è¶³ â†’ åŸ·è¡Œå®Œæ•´å·¥ä½œæµ:
       a. Tavily æœå°‹ + ç¶²é çˆ¬å– (ä¸€æ¬¡)
       b. è³‡æ–™èƒå–ä¸¦å­˜å…¥ Neo4j
       c. ç”Ÿæˆå ±å‘Š
    
    Returns:
        å®Œæ•´çš„å ±å‘Šè³‡æ–™
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°çµ±ä¸€åˆ†æè«‹æ±‚: {request.query}")
        
        # ============ æ­¥é©Ÿ 1: æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦ ============
        logger.info(f"ğŸ” æ­¥é©Ÿ 1/2: æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦")
        coverage = agent._check_database_coverage(request.query, [])
        
        logger.info(f"ğŸ“Š è³‡æ–™åº«ç‹€æ…‹: {coverage['entity_count']} å€‹å¯¦é«”, {coverage['relationship_count']} å€‹é—œä¿‚")
        
        # ============ æ­¥é©Ÿ 2: æ ¹æ“šè¦†è“‹åº¦æ±ºå®šè¡Œå‹• ============
        if coverage["has_sufficient_data"]:
            # âœ… è³‡æ–™å……è¶³ï¼Œç›´æ¥ç”Ÿæˆå ±å‘Š
            logger.info("âœ… è³‡æ–™åº«è³‡æ–™å……è¶³ï¼Œç›´æ¥ç”Ÿæˆå ±å‘Š")
            logger.info(f"ğŸ“ æ­¥é©Ÿ 2/2: ç”Ÿæˆå ±å‘Š")
            
            report_data = agent.report_generator.generate_comprehensive_report(
                query=request.query,
                search_results=[],
                use_neo4j=True
            )
            
            return {
                "status": "success",
                "query": request.query,
                "action": "generate_report",
                "report": report_data["report"],
                "sources": report_data["sources"],
                "workflow_steps": {
                    "database_entities": coverage["entity_count"],
                    "database_relationships": coverage["relationship_count"],
                    "action": "used_existing_data"
                },
                "generated_at": report_data["generated_at"]
            }
        
        else:
            # âŒ è³‡æ–™ä¸è¶³ï¼ŒåŸ·è¡Œå®Œæ•´å·¥ä½œæµ
            logger.info("âš ï¸ è³‡æ–™åº«è³‡æ–™ä¸è¶³ï¼ŒåŸ·è¡Œå®Œæ•´å·¥ä½œæµ")
            logger.info(f"ğŸ“ æ­¥é©Ÿ 2/2: æœå°‹ â†’ çˆ¬å– â†’ èƒå– â†’ ç”Ÿæˆå ±å‘Š")
            
            # æ§‹å»ºå·¥ä½œæµè«‹æ±‚
            workflow_request = {
                "action": "scrape_and_extract",
                "query": request.query,
                "reason": "è³‡æ–™åº«è³‡æ–™ä¸è¶³",
                "coverage": coverage,
                "urls_to_scrape": [],  # ç©ºåˆ—è¡¨ï¼Œè®“ workflow è‡ªå·±ç”¨ Tavily æœå°‹
                "search_results": []
            }
            
            # åŸ·è¡Œå·¥ä½œæµ
            final_result = await agent.orchestrate_workflow(workflow_request)
            
            return {
                "status": final_result.get("status", "success"),
                "query": request.query,
                "action": final_result.get("action", "scrape_and_extract"),
                "report": final_result.get("report", "ç„¡æ³•ç”Ÿæˆå ±å‘Š"),
                "sources": final_result.get("sources", {}),
                "workflow_steps": final_result.get("workflow_steps"),
                "generated_at": final_result.get("generated_at", "")
            }
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±æ•—: {e}", exc_info=True)
        return {
            "status": "error",
            "query": request.query,
            "action": "error",
            "report": f"# åˆ†æå¤±æ•—\n\næŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
            "sources": {},
            "generated_at": "",
            "error": str(e)
        }


@app.post("/orchestrate")
async def orchestrate_workflow(self, request: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ¹æ“š action åŸ·è¡Œç›¸æ‡‰çš„å·¥ä½œæµ
    """
    action = request.get("action")
    query = request.get("query")
    
    logger.info(f"ğŸ¬ é–‹å§‹åŸ·è¡Œå·¥ä½œæµ: {action}")
    
    try:
        if action == "generate_report":
            # ç›´æ¥ç”Ÿæˆå ±å‘Š
            search_results = request.get("search_results", [])
            report_data = self.report_generator.generate_comprehensive_report(
                query=query,
                search_results=search_results,
                use_neo4j=True
            )
            
            logger.info(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆ")
            return {
                "status": "success",
                "action": "generate_report",
                "report": report_data["report"],
                "query": query,
                "sources": report_data["sources"],
                "generated_at": report_data["generated_at"]
            }
            
        elif action == "scrape_and_extract":
            # åŸ·è¡Œå®Œæ•´æµç¨‹ï¼šTavily æœå°‹ + çˆ¬èŸ² -> èƒå– -> å„²å­˜ -> ç”Ÿæˆå ±å‘Š
            
            # æ­¥é©Ÿ 1: ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é 
            logger.info(f"   ğŸ” æ­¥é©Ÿ 1: ä½¿ç”¨ Tavily æœå°‹ä¸¦çˆ¬å–ç¶²é ")
            scraped_data = await self._search_and_scrape(query)
            
            if not scraped_data.get("results"):
                logger.warning("   âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç¶²é è³‡æ–™")
                # å³ä½¿æ²’æœ‰æ–°è³‡æ–™ï¼Œä¹Ÿå˜—è©¦ç”¨è³‡æ–™åº«ç”Ÿæˆå ±å‘Š
                report_data = self.report_generator.generate_comprehensive_report(
                    query=query,
                    search_results=[],
                    use_neo4j=True
                )
                return {
                    "status": "success",
                    "action": "scrape_and_extract",
                    "report": report_data["report"],
                    "query": query,
                    "sources": report_data["sources"],
                    "workflow_steps": {
                        "scraped_urls": 0,
                        "extracted_entities": 0,
                        "note": "æœªæ‰¾åˆ°æ–°è³‡æ–™ï¼Œä½¿ç”¨ç¾æœ‰è³‡æ–™åº«ç”Ÿæˆå ±å‘Š"
                    },
                    "generated_at": report_data["generated_at"]
                }
            
            # æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™ï¼ˆèƒå– agent æœƒè‡ªå‹•å­˜å…¥ Neo4jï¼‰
            logger.info(f"   ğŸ”¬ æ­¥é©Ÿ 2: èƒå–çµæ§‹åŒ–è³‡æ–™ä¸¦å­˜å…¥ Neo4j")
            extracted_data = await self._extract_data(query, scraped_data)
            
            # âœ… ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨èƒå–çµæœï¼Œä¸å†æŸ¥è©¢ Neo4j
            # å› ç‚ºèƒå– agent å·²ç¶“å­˜å…¥ Neo4jï¼Œæˆ‘å€‘ç›´æ¥ä½¿ç”¨è¿”å›çš„å¯¦é«”å’Œé—œä¿‚
            
            entities = extracted_data.get("entities", [])
            relationships = extracted_data.get("relationships", [])
            
            logger.info(f"   ğŸ“ æ­¥é©Ÿ 3: ä½¿ç”¨èƒå–çµæœç”Ÿæˆå ±å‘Š")
            
            # ç›´æ¥å‚³éèƒå–çš„å¯¦é«”å’Œé—œä¿‚çµ¦å ±å‘Šç”Ÿæˆå™¨
            report_data = self.report_generator.generate_report_from_extraction(
                query=query,
                entities=entities,
                relationships=relationships,
                search_results=scraped_data.get("results", [])
            )
            
            logger.info(f"âœ… å®Œæ•´å·¥ä½œæµåŸ·è¡Œå®Œæˆ")
            return {
                "status": "success",
                "action": "scrape_and_extract",
                "report": report_data["report"],
                "query": query,
                "sources": report_data["sources"],
                "workflow_steps": {
                    "scraped_urls": len(scraped_data.get("results", [])),
                    "extracted_entities": len(entities),
                    "extracted_relationships": len(relationships)
                },
                "generated_at": report_data["generated_at"]
            }
        
        else:
            raise ValueError(f"Unknown action: {action}")
            
    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        return {
            "status": "error",
            "action": action,
            "query": query,
            "error": str(e),
            "report": f"# å ±å‘Šç”Ÿæˆå¤±æ•—\n\næŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
        }


@app.post("/check-coverage")
async def check_database_coverage(request: Dict[str, Any]):
    """
    å–®ç¨æª¢æŸ¥è³‡æ–™åº«è¦†è“‹åº¦ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
    """
    try:
        query = request.get("query", "")
        results = request.get("results", [])
        coverage = agent._check_database_coverage(query, results)
        return coverage
    except Exception as e:
        logger.error(f"Error checking coverage: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
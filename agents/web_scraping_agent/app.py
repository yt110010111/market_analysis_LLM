from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
import logging
from agent import WebScrapingAgent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Web Scraping Agent API")

agent = WebScrapingAgent()


class ScrapeRequest(BaseModel):
    urls: List[str]
    query: str = ""


class ScrapeResponse(BaseModel):
    query: str
    total_urls: int
    successful: int
    failed: int
    results: List[Dict[str, Any]]
    timestamp: str


@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "web_scraping_agent",
        "version": "0.1.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "scrape": "/scrape (POST)"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "service": "web_scraping_agent",
        "timeout": agent.timeout,
        "max_retries": agent.max_retries
    }


@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_urls(request: ScrapeRequest):
    """
    çˆ¬å–æŒ‡å®šçš„ URL åˆ—è¡¨
    
    Args:
        urls: è¦çˆ¬å–çš„ URL åˆ—è¡¨
        query: ç›¸é—œçš„æŸ¥è©¢ï¼ˆå¯é¸ï¼‰
    
    Returns:
        çˆ¬å–çµæœ
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°çˆ¬å–è«‹æ±‚: {len(request.urls)} å€‹ URL")
        
        if not request.urls:
            raise HTTPException(status_code=400, detail="URL list is empty")
        
        # é™åˆ¶ä¸€æ¬¡æœ€å¤šçˆ¬å– 10 å€‹ URL
        if len(request.urls) > 10:
            logger.warning(f"âš ï¸ URL æ•¸é‡éå¤šï¼Œé™åˆ¶ç‚ºå‰ 10 å€‹")
            request.urls = request.urls[:10]
        
        # åŸ·è¡Œçˆ¬å–
        results = await agent.scrape_urls(request.urls, request.query)
        
        # å¯é¸ï¼šå„²å­˜ç‚º JSONï¼ˆåœ¨ç”Ÿç”¢ç’°å¢ƒå¯èƒ½è¦æ”¹ç‚ºè³‡æ–™åº«ï¼‰
        # agent.save_results_to_json(results, f"scraping_{results['timestamp']}.json")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/scrape/single")
async def scrape_single_url(url: str, query: str = ""):
    """
    çˆ¬å–å–®å€‹ URLï¼ˆä¾¿æ·ç«¯é»ï¼‰
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°å–®ä¸€ URL çˆ¬å–è«‹æ±‚: {url}")
        
        results = await agent.scrape_urls([url], query)
        
        if results["successful"] > 0:
            return results["results"][0]
        else:
            raise HTTPException(status_code=500, detail="Scraping failed")
            
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
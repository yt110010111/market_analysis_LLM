#agents/web_scraping_agent/app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
import logging
from agent import WebScrapingAgent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Web Scraping Agent API")

agent = WebScrapingAgent()


class ScrapeRequest(BaseModel):
    urls: List[str]
    query: str = ""
    dynamic_search: bool = True  # é è¨­å•Ÿç”¨å‹•æ…‹æœå°‹


class ScrapeResponse(BaseModel):
    query: str
    total_urls: int
    successful: int
    failed: int
    results: List[Dict[str, Any]]
    timestamp: str


@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "web_scraping_agent",
        "version": "0.1.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "scrape": "/scrape (POST)"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "service": "web_scraping_agent",
        "timeout": agent.timeout,
        "max_retries": agent.max_retries
    }


@app.post("/scrape", response_model=ScrapeResponse)
async def scrape_urls(request: ScrapeRequest):
    """
    çˆ¬å–æŒ‡å®šçš„ URL åˆ—è¡¨
    
    Args:
        urls: è¦çˆ¬å–çš„ URL åˆ—è¡¨
        query: ç›¸é—œçš„æŸ¥è©¢ï¼ˆå¯é¸ï¼‰
        dynamic_search: æ˜¯å¦ä½¿ç”¨ Tavily å‹•æ…‹æœå°‹æ›´å¤š URLï¼ˆé è¨­ Trueï¼‰
    
    Returns:
        çˆ¬å–çµæœ
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°çˆ¬å–è«‹æ±‚: {len(request.urls)} å€‹ URL, query='{request.query}', dynamic_search={request.dynamic_search}")
        
        # å¦‚æœæ—¢æ²’æœ‰ URL ä¹Ÿæ²’æœ‰ queryï¼Œç„¡æ³•è™•ç†
        if not request.urls and not request.query:
            raise HTTPException(
                status_code=400, 
                detail="éœ€è¦æä¾› URLs æˆ– queryï¼ˆå•Ÿç”¨ dynamic_search æ™‚ï¼‰"
            )
        
        # ä½¿ç”¨æä¾›çš„ URL æˆ–ç©ºåˆ—è¡¨
        urls = request.urls if request.urls else []
        
        # åŸ·è¡Œçˆ¬å–ï¼ˆå¯èƒ½åŒ…å«å‹•æ…‹æœå°‹ï¼‰
        results = await agent.scrape_urls(
            urls, 
            request.query, 
            dynamic_search=request.dynamic_search
        )
        
        return results
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/scrape/single")
async def scrape_single_url(url: str, query: str = ""):
    """
    çˆ¬å–å–®å€‹ URLï¼ˆä¾¿æ·ç«¯é»ï¼‰
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°å–®ä¸€ URL çˆ¬å–è«‹æ±‚: {url}")
        
        results = await agent.scrape_urls([url], query)
        
        if results["successful"] > 0:
            return results["results"][0]
        else:
            raise HTTPException(status_code=500, detail="Scraping failed")
            
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
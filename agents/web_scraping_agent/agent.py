#agents/web_scraping_agent/agent.py
import os
import logging
import asyncio
from typing import List, Dict, Any
from datetime import datetime
import httpx
from bs4 import BeautifulSoup
import json
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WebScrapingAgent:
    """
    ç¶²é çˆ¬èŸ²ä»£ç†ï¼šçˆ¬å–æŒ‡å®š URL çš„å…§å®¹
    æ”¯æ´å‹•æ…‹æœå°‹ï¼ˆä½¿ç”¨ Tavilyï¼‰ä¾†ç²å–æ›´å¤šç›¸é—œ URL
    """
    
    def __init__(self):
        self.timeout = int(os.getenv("SCRAPING_TIMEOUT", "30"))
        self.max_retries = int(os.getenv("SCRAPING_MAX_RETRIES", "3"))
        self.user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        self.tavily_api_key = os.getenv("TAVILY_API_KEY", "")
        
    async def scrape_urls(self, urls: List[str], query: str = "", dynamic_search: bool = False) -> Dict[str, Any]:
        """
        çˆ¬å–å¤šå€‹ URL çš„å…§å®¹
        
        Args:
            urls: è¦çˆ¬å–çš„ URL åˆ—è¡¨
            query: ç›¸é—œçš„æŸ¥è©¢ï¼ˆç”¨æ–¼ä¸Šä¸‹æ–‡ï¼‰
            dynamic_search: æ˜¯å¦ä½¿ç”¨ Tavily å‹•æ…‹æœå°‹æ›´å¤š URL
            
        Returns:
            çˆ¬å–çµæœçš„å­—å…¸
        """
        logger.info(f"ğŸ•·ï¸ é–‹å§‹çˆ¬å– {len(urls)} å€‹ URL")
        
        # å¦‚æœå•Ÿç”¨å‹•æ…‹æœå°‹ä¸”æœ‰ queryï¼Œä½¿ç”¨ Tavily ç²å–æ›´å¤š URL
        if dynamic_search and query and self.tavily_api_key:
            logger.info(f"ğŸ” ä½¿ç”¨ Tavily å‹•æ…‹æœå°‹: {query}")
            additional_urls = self._search_with_tavily(query, max_results=5)
            if additional_urls:
                logger.info(f"âœ… Tavily æ‰¾åˆ° {len(additional_urls)} å€‹é¡å¤– URL")
                urls = list(set(urls + additional_urls))  # åˆä½µä¸¦å»é‡
            else:
                logger.warning("âš ï¸ Tavily æœå°‹æœªè¿”å›çµæœ")
        
        results = []
        successful = 0
        failed = 0
        
        async with httpx.AsyncClient(timeout=self.timeout, follow_redirects=True) as client:
            tasks = [self._scrape_single_url(client, url, idx) for idx, url in enumerate(urls)]
            scrape_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in scrape_results:
            if isinstance(result, Exception):
                logger.error(f"âŒ çˆ¬å–å¤±æ•—: {result}")
                failed += 1
            elif result and result.get("success"):
                results.append(result)
                successful += 1
            else:
                failed += 1
        
        logger.info(f"âœ… çˆ¬å–å®Œæˆ: æˆåŠŸ {successful}, å¤±æ•— {failed}")
        
        return {
            "query": query,
            "total_urls": len(urls),
            "successful": successful,
            "failed": failed,
            "results": results,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    
    def _search_with_tavily(self, query: str, max_results: int = 5) -> List[str]:
        """
        ä½¿ç”¨ Tavily API æœå°‹ç›¸é—œ URL
        """
        try:
            response = requests.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": self.tavily_api_key,
                    "query": query,
                    "max_results": max_results,
                    "search_depth": "advanced",
                    "include_raw_content": False
                },
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            results = data.get("results", [])
            
            urls = [result.get("url") for result in results if result.get("url")]
            logger.info(f"ğŸ“‹ Tavily è¿”å› {len(urls)} å€‹ URL")
            
            return urls
            
        except Exception as e:
            logger.error(f"âŒ Tavily æœå°‹å¤±æ•—: {e}")
            return []
    
    async def _scrape_single_url(self, client: httpx.AsyncClient, url: str, idx: int) -> Dict[str, Any]:
        """
        çˆ¬å–å–®å€‹ URL
        """
        logger.info(f"ğŸ“„ [{idx+1}] çˆ¬å–: {url}")
        
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        }
        
        for attempt in range(self.max_retries):
            try:
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                
                # è§£æ HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ¨™é¡Œ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else ""
                
                # æå–ä¸»è¦å…§å®¹
                content = self._extract_main_content(soup)
                
                # æå– meta æè¿°
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                description = meta_desc.get('content', '') if meta_desc else ""
                
                # æå–æ‰€æœ‰æ®µè½æ–‡å­—
                paragraphs = soup.find_all(['p', 'article', 'section'])
                text_content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                
                # æˆªæ–·éé•·çš„å…§å®¹ï¼ˆä¿ç•™å‰ 5000 å­—å…ƒï¼‰
                if len(text_content) > 5000:
                    text_content = text_content[:5000] + "..."
                
                logger.info(f"âœ… [{idx+1}] æˆåŠŸ: {url} (é•·åº¦: {len(text_content)} å­—å…ƒ)")
                
                return {
                    "success": True,
                    "url": url,
                    "title": title_text,
                    "description": description,
                    "content": content,
                    "full_text": text_content,
                    "content_length": len(text_content),
                    "scraped_at": datetime.utcnow().isoformat() + "Z"
                }
                
            except httpx.HTTPStatusError as e:
                logger.warning(f"âš ï¸ [{idx+1}] HTTP éŒ¯èª¤ (å˜—è©¦ {attempt+1}/{self.max_retries}): {e.response.status_code}")
                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "url": url,
                        "error": f"HTTP {e.response.status_code}",
                        "error_type": "http_error"
                    }
                await asyncio.sleep(1)
                
            except httpx.TimeoutException:
                logger.warning(f"â±ï¸ [{idx+1}] è¶…æ™‚ (å˜—è©¦ {attempt+1}/{self.max_retries})")
                if attempt == self.max_retries - 1:
                    return {
                        "success": False,
                        "url": url,
                        "error": "Request timeout",
                        "error_type": "timeout"
                    }
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ [{idx+1}] éŒ¯èª¤: {str(e)}")
                return {
                    "success": False,
                    "url": url,
                    "error": str(e),
                    "error_type": "unknown"
                }
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """
        æå–ç¶²é çš„ä¸»è¦å…§å®¹
        å˜—è©¦æ‰¾åˆ° main, article æˆ–å…¶ä»–ä¸»è¦å…§å®¹æ¨™ç±¤
        """
        # å„ªå…ˆå°‹æ‰¾é€™äº›æ¨™ç±¤
        main_tags = ['main', 'article', '[role="main"]', '.content', '#content']
        
        for tag in main_tags:
            if tag.startswith('.') or tag.startswith('#') or tag.startswith('['):
                # CSS é¸æ“‡å™¨
                element = soup.select_one(tag)
            else:
                element = soup.find(tag)
            
            if element:
                text = element.get_text(separator='\n', strip=True)
                if len(text) > 100:  # ç¢ºä¿æœ‰è¶³å¤ çš„å…§å®¹
                    return text[:3000]  # é™åˆ¶é•·åº¦
        
        # å¦‚æœæ‰¾ä¸åˆ°ä¸»è¦å…§å®¹ï¼Œè¿”å› body çš„æ–‡å­—
        body = soup.find('body')
        if body:
            # ç§»é™¤ script å’Œ style æ¨™ç±¤
            for script in body(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            return body.get_text(separator='\n', strip=True)[:3000]
        
        return ""
    
    def save_results_to_json(self, results: Dict[str, Any], output_path: str = "scraping_results.json"):
        """
        å°‡çˆ¬å–çµæœå„²å­˜ç‚º JSON æª”æ¡ˆ
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ å„²å­˜å¤±æ•—: {e}")
            return False
import os
import json
import logging
import re
from typing import Dict, List, Any
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataExtractionAgent:
    """
    æ¥µé€Ÿç‰ˆæœ¬ - é‡å°æ…¢é€Ÿ LLM çš„æ¿€é€²å„ªåŒ–ï¼š
    1. å¤§å¹…æ¸›å°‘æ–‡æœ¬é•·åº¦ï¼ˆ3000 å­—ç¬¦ï¼‰
    2. æ¥µç°¡ Promptï¼ˆæœ€å°‘ tokenï¼‰
    3. å®¹éŒ¯è™•ç†ï¼ˆå³ä½¿éƒ¨åˆ†å¤±æ•—ä¹Ÿèƒ½ç¹¼çºŒï¼‰
    4. æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶
    5. é™ç´šç­–ç•¥ï¼ˆLLM æ…¢æ™‚ä½¿ç”¨è¦å‰‡æå–ï¼‰
    """

    def __init__(self):
        self.ollama_endpoint = os.getenv("OLLAMA_ENDPOINT", "http://ollama:11434")
        self.model_name = os.getenv("MODEL_NAME", "llama3.2:3b")
        self.max_docs = int(os.getenv("MAX_DOCS", "3"))  # æ¸›å°‘åˆ° 3 å€‹æ–‡æª”
        self.max_chars_per_doc = int(os.getenv("MAX_CHARS_PER_DOC", "3000"))  # å¤§å¹…æ¸›å°‘
        self.timeout = int(os.getenv("OLLAMA_TIMEOUT", "30"))  # é™ä½åˆ° 30 ç§’
        self.max_workers = int(os.getenv("MAX_WORKERS", "2"))  # æ¸›å°‘ä¸¦è¡Œæ•¸
        
        # å¦‚æœ LLM è¶…æ™‚ï¼Œä½¿ç”¨è¦å‰‡æå–
        self.use_fallback = True

    def extract_and_analyze(self, scraped_data: Dict[str, Any], query: str) -> Dict[str, Any]:
        results = scraped_data.get("results", [])
        if not results:
            return {"query": query, "status": "no_data", "entities": [], "summary": "ç„¡å¯åˆ†æè³‡æ–™"}

        logger.info(f"ğŸ”¬ é–‹å§‹è™•ç† {len(results)} ä»½æ–‡ä»¶ï¼Œç›®æ¨™ä¸»é¡Œ: {query}")

        all_entities = []
        all_relationships = []
        document_summaries = []
        
        success_count = 0
        timeout_count = 0

        # ä¸¦è¡Œè™•ç†ï¼ˆæ¸›å°‘ä¸¦è¡Œæ•¸é¿å…è³‡æºç«¶çˆ­ï¼‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_doc = {
                executor.submit(self._process_single_document, doc, query, idx): (doc, idx)
                for idx, doc in enumerate(results[:self.max_docs], start=1)
            }

            for future in as_completed(future_to_doc):
                doc, idx = future_to_doc[future]
                try:
                    result = future.result(timeout=35)  # æ¯å€‹æ–‡æª”æœ€å¤š 35 ç§’
                    
                    if result and result["entities"]:
                        all_entities.extend(result["entities"])
                        all_relationships.extend(result["relationships"])
                        document_summaries.append(result["summary_info"])
                        success_count += 1
                        logger.info(f"âœ… æ–‡æª” {idx} å®Œæˆ: {len(result['entities'])} å€‹å¯¦é«”")
                    elif result:
                        # LLM è¶…æ™‚ï¼Œä½¿ç”¨ fallback
                        logger.warning(f"âš ï¸ æ–‡æª” {idx} LLM è¶…æ™‚ï¼Œä½¿ç”¨è¦å‰‡æå–")
                        fallback_result = self._fallback_extraction(doc, query, idx)
                        all_entities.extend(fallback_result["entities"])
                        document_summaries.append(fallback_result["summary_info"])
                        timeout_count += 1
                        
                except TimeoutError:
                    logger.error(f"âŒ æ–‡æª” {idx} å®Œå…¨è¶…æ™‚")
                    timeout_count += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–‡æª” {idx} è™•ç†å¤±æ•—: {e}")
                    timeout_count += 1

        logger.info(f"ğŸ“Š è™•ç†çµæœ: {success_count} æˆåŠŸ, {timeout_count} è¶…æ™‚/å¤±æ•—")

        # å³ä½¿éƒ¨åˆ†å¤±æ•—ï¼Œä¹Ÿè¿”å›å·²æå–çš„çµæœ
        if not all_entities:
            # æœ€å¾Œçš„ä¿åº•ï¼šä½¿ç”¨è¦å‰‡æå–æ‰€æœ‰æ–‡æª”
            logger.warning("âš ï¸ æ‰€æœ‰ LLM æå–å¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡æå–")
            for idx, doc in enumerate(results[:self.max_docs], start=1):
                fallback_result = self._fallback_extraction(doc, query, idx)
                all_entities.extend(fallback_result["entities"])

        if not all_entities:
            return {
                "query": query, 
                "status": "empty", 
                "entities": [], 
                "summary": "è™•ç†è¶…æ™‚ï¼Œç„¡æ³•æå–è³‡è¨Š"
            }

        # å¿«é€Ÿå»é‡
        unique_entities = self._fast_deduplicate_entities(all_entities, query)
        unique_relationships = self._fast_deduplicate_relationships(all_relationships)

        overall_summary = self._generate_fast_summary(document_summaries, query)

        logger.info(f"âœ… æœ€çµ‚çµæœï¼š{len(unique_entities)} å€‹ç¨ç‰¹å¯¦é«”ï¼Œ{len(unique_relationships)} å€‹é—œä¿‚")

        return {
            "query": query,
            "entities": unique_entities,
            "relationships": unique_relationships,
            "document_summaries": document_summaries,
            "overall_summary": overall_summary,
            "status": "success" if success_count > 0 else "partial",
            "statistics": {
                "total_entities": len(unique_entities),
                "total_relationships": len(unique_relationships),
                "documents_processed": len(document_summaries),
                "success_count": success_count,
                "timeout_count": timeout_count
            }
        }

    def _process_single_document(self, doc: Dict[str, Any], query: str, idx: int) -> Dict[str, Any]:
        """è™•ç†å–®å€‹æ–‡æª”"""
        text = doc.get("full_text") or doc.get("content", "")
        title = doc.get("title", "")
        url = doc.get("url", "")
        
        # æ¥µç°¡æ¸…ç†
        cleaned = self._minimal_preprocess(text)
        # åªå–å‰ 3000 å­—ç¬¦
        sampled = cleaned[:self.max_chars_per_doc]

        if not sampled.strip():
            return None

        # å˜—è©¦ LLM æå–
        try:
            extraction = self._extract_with_llm(sampled, title, url, query)
            
            if not extraction or not extraction.get("entities"):
                # LLM å¤±æ•—ï¼Œä½¿ç”¨è¦å‰‡æå–
                return self._fallback_extraction(doc, query, idx)
            
            # å¯¬é¬†éæ¿¾
            relevant_entities = [
                e for e in extraction.get("entities", []) 
                if self._is_loosely_relevant(e.get("name", ""), e.get("description", ""), e.get("type", ""), query)
            ]

            return {
                "entities": relevant_entities,
                "relationships": extraction.get("relationships", []),
                "summary_info": {
                    "url": url,
                    "title": title,
                    "summary": extraction.get("summary", ""),
                    "entity_count": len(relevant_entities)
                }
            }
        except Exception as e:
            logger.warning(f"LLM æå–ç•°å¸¸: {e}ï¼Œä½¿ç”¨è¦å‰‡æå–")
            return self._fallback_extraction(doc, query, idx)

    # =========================
    # æ¥µç°¡æ–‡æœ¬è™•ç†
    # =========================

    def _minimal_preprocess(self, text: str) -> str:
        """æ¥µç°¡æ¸…ç†ï¼ˆæœ€å¿«é€Ÿåº¦ï¼‰"""
        if not text:
            return ""
        
        # åªåšæœ€åŸºæœ¬çš„æ¸…ç†ï¼Œä¸åšè¤‡é›œè™•ç†
        lines = [line.strip() for line in text.split("\n") if len(line.strip()) > 20]
        return "\n".join(lines[:100])  # æœ€å¤šä¿ç•™ 100 è¡Œ

    def _is_loosely_relevant(self, name: str, desc: str, entity_type: str, query: str) -> bool:
        """å¯¬é¬†éæ¿¾"""
        if not name or len(name) < 2:
            return False
        
        q_lower = query.lower()
        name_lower = name.lower()
        desc_lower = desc.lower() if desc else ""
        
        # ç›´æ¥ç›¸é—œ
        if q_lower in name_lower or q_lower in desc_lower:
            return True
        
        # é‡è¦é¡å‹
        if entity_type in ["å…¬å¸/çµ„ç¹”", "äººç‰©", "ç”¢å“/æœå‹™", "ç«¶çˆ­å°æ‰‹", "åˆä½œå¤¥ä¼´"]:
            return True
        
        # æœ‰æè¿°
        if len(desc_lower) > 10:
            return True
        
        return False

    # =========================
    # æ¥µç°¡ LLM èª¿ç”¨
    # =========================

    def _extract_with_llm(self, text: str, title: str, url: str, query: str) -> Dict[str, Any]:
        """æ¥µç°¡ Promptï¼Œæœ€å° token æ•¸"""
        # è¶…çŸ­ Prompt
        prompt = f"""æå–ã€Œ{query}ã€ç›¸é—œå¯¦é«”å’Œé—œä¿‚ã€‚

æ–‡æª”ï¼š{title}
{text[:2000]}

JSONæ ¼å¼ï¼š
{{"entities":[{{"name":"åç¨±","type":"é¡å‹","description":"èªªæ˜"}}],"relationships":[{{"source":"A","target":"B","relation":"é—œä¿‚"}}],"summary":"æ‘˜è¦"}}

é¡å‹ï¼šå…¬å¸ã€äººç‰©ã€ç”¢å“ã€ç«¶çˆ­å°æ‰‹ã€åˆä½œå¤¥ä¼´ã€‚"""

        response = self._call_ollama_quick(prompt)
        return self._parse_llm_response(response, query, title, url)

    def _call_ollama_quick(self, prompt: str) -> str:
        """å¿«é€Ÿ Ollama èª¿ç”¨"""
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "format": "json",
            "options": {
                "temperature": 0,
                "num_predict": 1000,  # å¤§å¹…é™åˆ¶è¼¸å‡º
                "top_k": 10,
                "top_p": 0.5
            }
        }

        try:
            response = requests.post(
                f"{self.ollama_endpoint}/api/generate", 
                json=payload, 
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json().get("response", "")
        except requests.Timeout:
            logger.warning(f"â±ï¸ Ollama è¶…æ™‚ï¼ˆ{self.timeout}sï¼‰")
            return None
        except Exception as e:
            logger.error(f"âŒ Ollama éŒ¯èª¤: {e}")
            return None

    def _parse_llm_response(self, text: str, query: str, source_title: str, source_url: str) -> Dict[str, Any]:
        """è§£æ LLM å›æ‡‰"""
        if not text:
            return None
        
        try:
            text = re.sub(r'```(json)?\s*', '', text)
            match = re.search(r'\{.*\}', text, re.DOTALL)
            json_str = match.group(0) if match else text
            
            parsed = json.loads(json_str)
            
            entities = parsed.get("entities", [])
            for e in entities:
                e["source_title"] = source_title
                e["source_url"] = source_url
                e.setdefault("type", "æœªåˆ†é¡")
                e.setdefault("description", "")
            
            return {
                "entities": entities,
                "relationships": parsed.get("relationships", []),
                "summary": parsed.get("summary", "")
            }
        except Exception as e:
            logger.warning(f"è§£æå¤±æ•—: {e}")
            return None

    # =========================
    # Fallback è¦å‰‡æå–
    # =========================

    def _fallback_extraction(self, doc: Dict[str, Any], query: str, idx: int) -> Dict[str, Any]:
        """
        ç•¶ LLM å¤±æ•—æ™‚çš„è¦å‰‡æå–ï¼ˆä¿åº•æ–¹æ¡ˆï¼‰
        ä½¿ç”¨ç°¡å–®çš„é—œéµå­—å’Œæ¨¡å¼åŒ¹é…
        """
        text = doc.get("full_text") or doc.get("content", "")
        title = doc.get("title", "")
        url = doc.get("url", "")
        
        entities = []
        
        # è¦å‰‡ 1ï¼šå¾æ¨™é¡Œæå–ä¸»é«”
        if title:
            entities.append({
                "name": query.upper(),
                "type": "å…¬å¸/çµ„ç¹”",
                "description": f"ä¾†è‡ªæ–‡æª”æ¨™é¡Œï¼š{title}",
                "source_title": title,
                "source_url": url
            })
        
        # è¦å‰‡ 2ï¼šæŸ¥æ‰¾å¤§å¯«è©çµ„ï¼ˆå¯èƒ½æ˜¯å…¬å¸åï¼‰
        capitalized_words = re.findall(r'\b[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+)*\b', text[:1000])
        for word in set(capitalized_words[:10]):  # æœ€å¤šå– 10 å€‹
            if len(word) > 2 and word.lower() != query.lower():
                entities.append({
                    "name": word,
                    "type": "å…¬å¸/çµ„ç¹”",
                    "description": f"åœ¨æ–‡æª”ä¸­æåŠ",
                    "source_title": title,
                    "source_url": url
                })
        
        # è¦å‰‡ 3ï¼šæŸ¥æ‰¾å¸¸è¦‹è·ä½ï¼ˆCEOã€CFO ç­‰ï¼‰
        positions = re.findall(r'(CEO|CFO|CTO|President|Chairman|Director|Founder)', text[:2000], re.IGNORECASE)
        if positions:
            entities.append({
                "name": f"{query} Leadership",
                "type": "äººç‰©",
                "description": f"æ–‡æª”ä¸­æåˆ°è·ä½ï¼š{', '.join(set(positions))}",
                "source_title": title,
                "source_url": url
            })
        
        logger.info(f"ğŸ”§ æ–‡æª” {idx} ä½¿ç”¨è¦å‰‡æå–: {len(entities)} å€‹å¯¦é«”")
        
        return {
            "entities": entities,
            "relationships": [],
            "summary_info": {
                "url": url,
                "title": title,
                "summary": f"è¦å‰‡æå–ï¼š{title}",
                "entity_count": len(entities)
            }
        }

    # =========================
    # å¿«é€Ÿå»é‡
    # =========================

    def _fast_deduplicate_entities(self, entities: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """å¿«é€Ÿå»é‡"""
        if not entities:
            return []
        
        seen = set()
        unique = []
        
        for e in entities:
            name = e.get("name", "").strip()
            if not name:
                continue
            
            key = re.sub(r'\s+', '', name.lower())
            
            if key not in seen:
                seen.add(key)
                unique.append(e)
        
        # ç°¡å–®æ’åº
        q = query.lower()
        unique.sort(key=lambda x: (
            q not in x.get("name", "").lower(),
            x.get("name", "")
        ))
        
        return unique

    def _fast_deduplicate_relationships(self, relationships: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¿«é€Ÿå»é‡é—œä¿‚"""
        seen = set()
        unique = []
        
        for r in relationships:
            key = (
                r.get("source", "").strip().lower(),
                r.get("relation", "").strip().lower(),
                r.get("target", "").strip().lower()
            )
            
            if key not in seen and all(key):
                seen.add(key)
                unique.append(r)
        
        return unique

    def _generate_fast_summary(self, document_summaries: List[Dict[str, Any]], query: str) -> str:
        """å¿«é€Ÿæ‘˜è¦"""
        if not document_summaries:
            return f"é—œæ–¼ {query} çš„è³‡è¨Šæå–å®Œæˆ"
        
        titles = [d.get("title", "") for d in document_summaries if d.get("title")]
        
        if titles:
            return f"å·²å¾ {len(document_summaries)} å€‹ä¾†æºæå–é—œæ–¼ {query} çš„è³‡è¨Šï¼ŒåŒ…æ‹¬ï¼š{', '.join(titles[:2])} ç­‰ã€‚"
        else:
            return f"å·²å¾ {len(document_summaries)} å€‹ä¾†æºæå–é—œæ–¼ {query} çš„è³‡è¨Šã€‚"
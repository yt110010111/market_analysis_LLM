import os
import json
import logging
from typing import Dict, List, Any
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataExtractionAgent:
    """
    è³‡æ–™èƒå–ä»£ç†ï¼šä½¿ç”¨ Ollama åˆ†æçˆ¬å–çš„å…§å®¹ï¼Œæå–é—œéµè³‡è¨Šå’Œé—œè¯
    """
    
    def __init__(self):
        self.ollama_endpoint = os.getenv("OLLAMA_ENDPOINT", "http://ollama:11434")
        self.model_name = os.getenv("MODEL_NAME", "llama3.2:3b")
        
    def extract_and_analyze(self, scraped_data: Dict[str, Any], query: str = "") -> Dict[str, Any]:
        """
        åˆ†æçˆ¬å–çš„è³‡æ–™ï¼Œæå–é—œéµè³‡è¨Šå’Œå¯¦é«”é—œè¯
        
        Args:
            scraped_data: web_scraping_agent çš„è¼¸å‡º
            query: åŸå§‹æŸ¥è©¢
            
        Returns:
            æå–çš„å¯¦é«”ã€é—œä¿‚å’Œæ‘˜è¦
        """
        logger.info(f"ğŸ”¬ é–‹å§‹åˆ†æ {len(scraped_data.get('results', []))} å€‹æ–‡æª”")
        
        results = scraped_data.get('results', [])
        if not results:
            logger.warning("âš ï¸ æ²’æœ‰å¯åˆ†æçš„è³‡æ–™")
            return {
                "query": query,
                "entities": [],
                "relationships": [],
                "summary": "ç„¡è³‡æ–™å¯åˆ†æ",
                "status": "no_data"
            }
        
        # åˆ†ææ¯å€‹æ–‡æª”
        all_entities = []
        all_relationships = []
        document_summaries = []
        
        for idx, doc in enumerate(results):
            if not doc.get("success"):
                continue
                
            logger.info(f"ğŸ“„ åˆ†ææ–‡æª” {idx+1}/{len(results)}: {doc.get('title', 'Untitled')}")
            
            # æå–å¯¦é«”å’Œé—œä¿‚
            extraction = self._extract_entities_and_relationships(doc, query)
            
            all_entities.extend(extraction.get("entities", []))
            all_relationships.extend(extraction.get("relationships", []))
            document_summaries.append({
                "url": doc.get("url"),
                "title": doc.get("title"),
                "summary": extraction.get("summary", "")
            })
        
        # å»é‡å¯¦é«”ï¼ˆåŸºæ–¼åç¨±ï¼‰
        unique_entities = self._deduplicate_entities(all_entities)
        
        # ç”Ÿæˆæ•´é«”æ‘˜è¦
        overall_summary = self._generate_overall_summary(document_summaries, query)
        
        logger.info(f"âœ… åˆ†æå®Œæˆ: å¯¦é«” {len(unique_entities)} å€‹, é—œä¿‚ {len(all_relationships)} å€‹")
        
        return {
            "query": query,
            "total_documents": len(results),
            "entities": unique_entities,
            "relationships": all_relationships,
            "document_summaries": document_summaries,
            "overall_summary": overall_summary,
            "status": "success"
        }
    
    def _extract_entities_and_relationships(self, doc: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        å¾å–®å€‹æ–‡æª”ä¸­æå–å¯¦é«”å’Œé—œä¿‚
        """
        content = doc.get("full_text", "") or doc.get("content", "")
        title = doc.get("title", "")
        
        # æˆªæ–·éé•·çš„å…§å®¹
        if len(content) > 3000:
            content = content[:3000]
        
        prompt = f"""åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–é—œéµå¯¦é«”å’Œå®ƒå€‘ä¹‹é–“çš„é—œä¿‚ã€‚

æŸ¥è©¢ä¸»é¡Œ: {query}
æ–‡æª”æ¨™é¡Œ: {title}

æ–‡æª”å…§å®¹:
{content}

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ï¼š
1. entities: å¯¦é«”åˆ—è¡¨ï¼Œæ¯å€‹å¯¦é«”åŒ…å« name (åç¨±), type (é¡å‹: äººç‰©/çµ„ç¹”/ç”¢å“/æ¦‚å¿µ/åœ°é»), description (ç°¡çŸ­æè¿°)
2. relationships: é—œä¿‚åˆ—è¡¨ï¼Œæ¯å€‹é—œä¿‚åŒ…å« source (ä¾†æºå¯¦é«”), target (ç›®æ¨™å¯¦é«”), relation (é—œä¿‚é¡å‹), description (æè¿°)
3. summary: é€™ç¯‡æ–‡æª”çš„ç°¡çŸ­æ‘˜è¦ï¼ˆ2-3å¥è©±ï¼‰

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š
{{
  "entities": [
    {{"name": "å¯¦é«”åç¨±", "type": "é¡å‹", "description": "æè¿°"}}
  ],
  "relationships": [
    {{"source": "ä¾†æº", "target": "ç›®æ¨™", "relation": "é—œä¿‚", "description": "æè¿°"}}
  ],
  "summary": "æ‘˜è¦æ–‡å­—"
}}
"""
        
        try:
            response = self._call_ollama(prompt)
            
            # è§£æ JSON
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç¢¼å¡Š
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.startswith("```"):
                response = response[3:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()
            
            extracted = json.loads(response)
            
            # ç‚ºå¯¦é«”æ·»åŠ ä¾†æº
            for entity in extracted.get("entities", []):
                entity["source_url"] = doc.get("url")
                entity["source_title"] = title
            
            return extracted
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±æ•—: {e}")
            logger.debug(f"åŸå§‹å›æ‡‰: {response[:500]}")
            
            # è¿”å›ç©ºçµæœ
            return {
                "entities": [],
                "relationships": [],
                "summary": "ç„¡æ³•è§£ææ–‡æª”å…§å®¹"
            }
        except Exception as e:
            logger.error(f"âŒ æå–å¤±æ•—: {e}")
            return {
                "entities": [],
                "relationships": [],
                "summary": "æå–éç¨‹ç™¼ç”ŸéŒ¯èª¤"
            }
    
    def _deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é™¤é‡è¤‡çš„å¯¦é«”ï¼ˆåŸºæ–¼åç¨±ï¼‰
        """
        seen = {}
        for entity in entities:
            name = entity.get("name", "").lower()
            if name and name not in seen:
                seen[name] = entity
            elif name:
                # åˆä½µä¾†æºè³‡è¨Š
                if "sources" not in seen[name]:
                    seen[name]["sources"] = [seen[name].get("source_url")]
                if entity.get("source_url") not in seen[name]["sources"]:
                    seen[name]["sources"].append(entity.get("source_url"))
        
        return list(seen.values())
    
    def _generate_overall_summary(self, document_summaries: List[Dict[str, Any]], query: str) -> str:
        """
        ç”Ÿæˆæ‰€æœ‰æ–‡æª”çš„æ•´é«”æ‘˜è¦
        """
        if not document_summaries:
            return "ç„¡å¯ç”¨è³‡æ–™"
        
        summaries_text = "\n\n".join([
            f"ä¾†æº {idx+1} ({doc['title']}): {doc['summary']}"
            for idx, doc in enumerate(document_summaries)
        ])
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹å¤šå€‹ä¾†æºçš„æ‘˜è¦ï¼Œç”Ÿæˆä¸€å€‹æ•´åˆæ€§çš„ç¸½çµï¼Œå›ç­”æŸ¥è©¢ä¸»é¡Œã€‚

æŸ¥è©¢ä¸»é¡Œ: {query}

å„ä¾†æºæ‘˜è¦:
{summaries_text}

è«‹æä¾›ä¸€å€‹æ¸…æ™°ã€é€£è²«çš„ç¸½çµï¼ˆ3-5 å¥è©±ï¼‰ï¼Œæ•´åˆæ‰€æœ‰ä¾†æºçš„é—œéµè³‡è¨Šï¼š
"""
        
        try:
            overall_summary = self._call_ollama(prompt)
            return overall_summary.strip()
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç¸½çµå¤±æ•—: {e}")
            return "ç„¡æ³•ç”Ÿæˆæ•´é«”æ‘˜è¦"
    
    def _call_ollama(self, prompt: str, max_tokens: int = 2000) -> str:
        """
        å‘¼å« Ollama API
        """
        try:
            response = requests.post(
                f"{self.ollama_endpoint}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # è¼ƒä½çš„æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„è¼¸å‡º
                        "num_predict": max_tokens
                    }
                },
                timeout=60
            )
            response.raise_for_status()
            return response.json().get("response", "")
        except Exception as e:
            logger.error(f"âŒ Ollama å‘¼å«å¤±æ•—: {e}")
            raise
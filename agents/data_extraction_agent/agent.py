#agents/data_extraction_agent/agent.py
import os
import json
import logging
import re
from typing import Dict, List, Any, Set, Tuple
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataExtractionAgent:
    """
    GPU åŠ é€Ÿè¶…å¼·ç‰ˆ - æœ€å¤§åŒ–å¯¦é«”å’Œé—œè¯æå–ï¼š
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. å¤šè¼ªæå– - ç”¨ä¸åŒè§’åº¦æå–åŒä¸€æ–‡æª”ï¼ˆ3è¼ªï¼‰
    2. å¤§æ–‡æœ¬åˆ†å¡Š - å°‡é•·æ–‡æª”åˆ‡åˆ†æˆå¤šå€‹å¡Šï¼Œæ¯å¡Šç¨ç«‹æå–
    3. é—œä¿‚æŒ–æ˜å¢å¼· - å°ˆé–€çš„é—œä¿‚æå–è¼ªæ¬¡
    4. å¯¦é«”æ“´å±• - åŸºæ–¼å·²æå–å¯¦é«”é€²è¡ŒäºŒæ¬¡æ“´å±•
    5. äº¤å‰é©—è­‰ - å¤šå€‹æ–‡æª”é–“çš„å¯¦é«”äº¤å‰å¼•ç”¨
    6. æ·±åº¦ä¸Šä¸‹æ–‡ - ç‚ºæ¯å€‹å¯¦é«”æå–è±å¯Œçš„ä¸Šä¸‹æ–‡
    """

    def __init__(self):
        self.ollama_endpoint = os.getenv("OLLAMA_ENDPOINT", "http://ollama:11434")
        self.model_name = os.getenv("MODEL_NAME", "llama3.2:3b")
        self.max_docs = int(os.getenv("MAX_DOCS", "10"))  # å¢åŠ åˆ° 10 å€‹æ–‡æª”
        self.chunk_size = int(os.getenv("CHUNK_SIZE", "4000"))  # æ¯å€‹å¡Š 4000 å­—ç¬¦
        self.max_chunks_per_doc = int(os.getenv("MAX_CHUNKS_PER_DOC", "5"))  # æ¯å€‹æ–‡æª”æœ€å¤š 5 å€‹å¡Š
        self.timeout = int(os.getenv("OLLAMA_TIMEOUT", "60"))
        self.max_workers = int(os.getenv("MAX_WORKERS", "5"))  # GPU æ”¯æŒæ›´å¤šä¸¦è¡Œ
        
        # å¤šè¼ªæå–é…ç½®
        self.enable_multi_pass = True  # å•Ÿç”¨å¤šè¼ªæå–
        self.enable_relationship_mining = True  # å•Ÿç”¨æ·±åº¦é—œä¿‚æŒ–æ˜
        self.enable_entity_expansion = True  # å•Ÿç”¨å¯¦é«”æ“´å±•
        
        # å¯¦é«”é¡å‹ï¼ˆæ›´ç´°ç·»çš„åˆ†é¡ï¼‰
        self.entity_types = {
            "organization": ["å…¬å¸", "çµ„ç¹”", "æ©Ÿæ§‹", "åœ˜éšŠ", "éƒ¨é–€", "å­å…¬å¸"],
            "person": ["å‰µå§‹äºº", "CEO", "é«˜ç®¡", "è‘£äº‹", "å“¡å·¥", "é¡§å•"],
            "product": ["ç”¢å“", "æœå‹™", "å¹³å°", "æ‡‰ç”¨", "è§£æ±ºæ–¹æ¡ˆ"],
            "technology": ["æŠ€è¡“", "ç®—æ³•", "æ¡†æ¶", "å·¥å…·", "å”è­°", "æ¨™æº–"],
            "competitor": ["ç«¶çˆ­å°æ‰‹", "æ›¿ä»£å“", "åŒæ¥­"],
            "partner": ["åˆä½œå¤¥ä¼´", "ä¾›æ‡‰å•†", "å®¢æˆ¶", "æˆ°ç•¥è¯ç›Ÿ"],
            "investor": ["æŠ•è³‡è€…", "å‰µæŠ•", "å¤©ä½¿æŠ•è³‡äºº", "ç§å‹ŸåŸºé‡‘"],
            "event": ["èè³‡", "æ”¶è³¼", "ä¸Šå¸‚", "ç™¼å¸ƒ", "çé …", "é‡Œç¨‹ç¢‘"],
            "metric": ["ç‡Ÿæ”¶", "ç”¨æˆ¶æ•¸", "å¸‚å€¼", "ä¼°å€¼", "å¢é•·ç‡", "å¸‚å ´ä»½é¡"],
            "location": ["ç¸½éƒ¨", "è¾¦å…¬å®¤", "å¸‚å ´", "åœ°å€"],
            "concept": ["ç­–ç•¥", "é¡˜æ™¯", "ä½¿å‘½", "åƒ¹å€¼ä¸»å¼µ", "å•†æ¥­æ¨¡å¼"]
        }
        
        # é—œä¿‚é¡å‹ï¼ˆæ›´è±å¯Œçš„é—œä¿‚ï¼‰
        self.relationship_types = [
            "å‰µç«‹", "é ˜å°", "ä»»è·æ–¼", "æŠ•è³‡", "è¢«æŠ•è³‡", 
            "æ”¶è³¼", "è¢«æ”¶è³¼", "åˆä½œ", "ç«¶çˆ­", "ä¾›æ‡‰",
            "ä½¿ç”¨", "é–‹ç™¼", "ç™¼å¸ƒ", "æ”¯æŒ", "é›†æˆ",
            "æ“æœ‰", "éš¸å±¬æ–¼", "ä½æ–¼", "æœå‹™æ–¼", "å½±éŸ¿",
            "è¡ç”Ÿè‡ª", "åŸºæ–¼", "å„ªæ–¼", "åŠ£æ–¼", "ç›¸ä¼¼æ–¼"
        ]

    def extract_and_analyze(self, scraped_data: Dict[str, Any], query: str) -> Dict[str, Any]:
        results = scraped_data.get("results", [])
        if not results:
            return {"query": query, "status": "no_data", "entities": [], "summary": "ç„¡å¯åˆ†æè³‡æ–™"}

        logger.info(f"ğŸš€ GPU åŠ é€Ÿæ¨¡å¼ï¼šé–‹å§‹æ·±åº¦è™•ç† {len(results)} ä»½æ–‡ä»¶ï¼Œç›®æ¨™ä¸»é¡Œ: {query}")

        # ========== éšæ®µ 1ï¼šå¤šè¼ªä¸¦è¡Œæå– ==========
        all_entities = []
        all_relationships = []
        document_summaries = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ–‡æª”è™•ç†ä»»å‹™
            futures = [
                executor.submit(self._deep_process_document, doc, query, idx)
                for idx, doc in enumerate(results[:self.max_docs], start=1)
            ]
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=180)  # æ¯å€‹æ–‡æª”æœ€å¤š 3 åˆ†é˜
                    if result:
                        all_entities.extend(result["entities"])
                        all_relationships.extend(result["relationships"])
                        document_summaries.append(result["summary_info"])
                        logger.info(f"âœ… æ–‡æª”è™•ç†å®Œæˆ: {len(result['entities'])} å¯¦é«”, {len(result['relationships'])} é—œä¿‚")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–‡æª”è™•ç†å¤±æ•—: {e}")

        if not all_entities:
            return {
                "query": query,
                "status": "empty",
                "entities": [],
                "relationships": [],
                "summary": "æœªèƒ½æå–å‡ºè³‡è¨Š"
            }

        logger.info(f"ğŸ“Š éšæ®µ 1 å®Œæˆ: {len(all_entities)} å€‹åŸå§‹å¯¦é«”, {len(all_relationships)} å€‹é—œä¿‚")

        # ========== éšæ®µ 2ï¼šå¯¦é«”å»é‡èˆ‡åˆä½µ ==========
        unique_entities = self._advanced_deduplicate_entities(all_entities, query)
        logger.info(f"ğŸ“Š å»é‡å¾Œ: {len(unique_entities)} å€‹ç¨ç‰¹å¯¦é«”")

        # ========== éšæ®µ 3ï¼šå¯¦é«”æ“´å±•ï¼ˆåŸºæ–¼å·²æœ‰å¯¦é«”æŒ–æ˜æ›´å¤šé—œè¯ï¼‰==========
        if self.enable_entity_expansion and len(unique_entities) > 5:
            expanded_entities = self._expand_entities(unique_entities, all_entities, query)
            unique_entities.extend(expanded_entities)
            unique_entities = self._advanced_deduplicate_entities(unique_entities, query)
            logger.info(f"ğŸ“Š æ“´å±•å¾Œ: {len(unique_entities)} å€‹å¯¦é«”")

        # ========== éšæ®µ 4ï¼šé—œä¿‚å»é‡èˆ‡æ¨æ–· ==========
        unique_relationships = self._advanced_deduplicate_relationships(all_relationships)
        
        # æ¨æ–·éš±å«é—œä¿‚ï¼ˆåŸºæ–¼å·²æœ‰å¯¦é«”å’Œé—œä¿‚ï¼‰
        inferred_relationships = self._infer_relationships(unique_entities, unique_relationships)
        unique_relationships.extend(inferred_relationships)
        unique_relationships = self._advanced_deduplicate_relationships(unique_relationships)
        
        logger.info(f"ğŸ“Š é—œä¿‚è™•ç†å®Œæˆ: {len(unique_relationships)} å€‹ç¨ç‰¹é—œä¿‚")

        # ========== éšæ®µ 5ï¼šç”Ÿæˆæ•´é«”æ‘˜è¦ ==========
        overall_summary = self._generate_comprehensive_summary(
            unique_entities, 
            unique_relationships, 
            document_summaries, 
            query
        )

        # ========== éšæ®µ 6ï¼šå¯¦é«”æ’åºèˆ‡è©•åˆ† ==========
        scored_entities = self._score_and_rank_entities(unique_entities, unique_relationships, query)

        logger.info(f"ğŸ‰ æœ€çµ‚çµæœï¼š{len(scored_entities)} å€‹å¯¦é«”ï¼Œ{len(unique_relationships)} å€‹é—œä¿‚")

        return {
            "query": query,
            "entities": scored_entities,
            "relationships": unique_relationships,
            "document_summaries": document_summaries,
            "overall_summary": overall_summary,
            "status": "success",
            "statistics": {
                "total_entities": len(scored_entities),
                "total_relationships": len(unique_relationships),
                "documents_processed": len(document_summaries),
                "entity_types": self._count_entity_types(scored_entities),
                "relationship_types": self._count_relationship_types(unique_relationships)
            }
        }

    # =========================
    # æ·±åº¦æ–‡æª”è™•ç†
    # =========================

    def _deep_process_document(self, doc: Dict[str, Any], query: str, idx: int) -> Dict[str, Any]:
        """æ·±åº¦è™•ç†å–®å€‹æ–‡æª”ï¼ˆå¤šè¼ªã€å¤šå¡Šï¼‰"""
        text = doc.get("full_text") or doc.get("content", "")
        title = doc.get("title", "")
        url = doc.get("url", "")
        
        if not text:
            return None
        
        logger.info(f"ğŸ“„ é–‹å§‹è™•ç†æ–‡æª” {idx}: {title[:50]}...")
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self._smart_clean_text(text)
        
        # å°‡æ–‡æœ¬åˆ‡åˆ†æˆå¤šå€‹å¡Š
        chunks = self._split_into_chunks(cleaned_text, self.chunk_size, overlap=500)
        chunks = chunks[:self.max_chunks_per_doc]
        
        logger.info(f"   åˆ‡åˆ†ç‚º {len(chunks)} å€‹å¡Š")
        
        all_entities = []
        all_relationships = []
        
        # ===== ç¬¬ 1 è¼ªï¼šåŸºç¤å¯¦é«”æå– =====
        for chunk_idx, chunk in enumerate(chunks, start=1):
            extraction = self._extract_entities_basic(chunk, title, url, query, chunk_idx)
            if extraction:
                all_entities.extend(extraction.get("entities", []))
                all_relationships.extend(extraction.get("relationships", []))
        
        logger.info(f"   ç¬¬ 1 è¼ªå®Œæˆ: {len(all_entities)} å¯¦é«”")
        
        # ===== ç¬¬ 2 è¼ªï¼šæ·±åº¦é—œä¿‚æŒ–æ˜ =====
        if self.enable_relationship_mining and len(all_entities) > 3:
            for chunk_idx, chunk in enumerate(chunks[:3], start=1):  # åªå°å‰ 3 å€‹å¡Šåšæ·±åº¦æŒ–æ˜
                deep_relationships = self._extract_relationships_deep(
                    chunk, title, url, query, all_entities
                )
                if deep_relationships:
                    all_relationships.extend(deep_relationships)
        
        logger.info(f"   ç¬¬ 2 è¼ªå®Œæˆ: {len(all_relationships)} é—œä¿‚")
        
        # ===== ç¬¬ 3 è¼ªï¼šä¸Šä¸‹æ–‡å¢å¼· =====
        if len(all_entities) > 0:
            enhanced_entities = self._enhance_entity_context(
                all_entities, cleaned_text, title, url
            )
            all_entities = enhanced_entities
        
        logger.info(f"   ç¬¬ 3 è¼ªå®Œæˆ: å¯¦é«”ä¸Šä¸‹æ–‡å·²å¢å¼·")
        
        # ç”Ÿæˆæ–‡æª”æ‘˜è¦
        summary = self._generate_document_summary(all_entities, all_relationships, title, query)
        
        return {
            "entities": all_entities,
            "relationships": all_relationships,
            "summary_info": {
                "url": url,
                "title": title,
                "summary": summary,
                "entity_count": len(all_entities),
                "relationship_count": len(all_relationships)
            }
        }

    # =========================
    # æ–‡æœ¬è™•ç†
    # =========================

    def _smart_clean_text(self, text: str) -> str:
        """æ™ºèƒ½æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤å¸¸è¦‹çš„ç¶²é å™ªéŸ³
        noise_patterns = [
            r'cookie\s+policy.*?(?:\n|$)',
            r'privacy\s+policy.*?(?:\n|$)',
            r'terms\s+of\s+service.*?(?:\n|$)',
            r'subscribe.*?newsletter.*?(?:\n|$)',
            r'related\s+articles.*?(?:\n|$)',
        ]
        
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ä¿ç•™æœ‰æ„ç¾©çš„æ®µè½ï¼ˆè‡³å°‘ 50 å­—ç¬¦ï¼‰
        paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]
        
        return '\n\n'.join(paragraphs)

    def _split_into_chunks(self, text: str, chunk_size: int, overlap: int = 0) -> List[str]:
        """å°‡æ–‡æœ¬åˆ‡åˆ†æˆé‡ç–Šçš„å¡Š"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            # å˜—è©¦åœ¨å¥å­é‚Šç•Œåˆ‡åˆ†
            if end < len(text):
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                split_point = max(last_period, last_newline)
                
                if split_point > chunk_size * 0.7:  # è‡³å°‘ä¿ç•™ 70%
                    chunk = chunk[:split_point + 1]
                    end = start + split_point + 1
            
            chunks.append(chunk)
            start = end - overlap  # é‡ç–Šéƒ¨åˆ†
        
        return chunks

    # =========================
    # LLM æå–ï¼ˆå¤šç¨®ç­–ç•¥ï¼‰
    # =========================

    def _extract_entities_basic(self, text: str, title: str, url: str, query: str, chunk_idx: int) -> Dict[str, Any]:
        """åŸºç¤å¯¦é«”æå–ï¼ˆå»£æ³›ä¸”å…¨é¢ï¼‰"""
        
        prompt = f"""ä½ æ˜¯çŸ¥è­˜åœ–è­œæ§‹å»ºå°ˆå®¶ã€‚è«‹å¾æ–‡æœ¬ä¸­æå–èˆ‡ã€Œ{query}ã€ç›¸é—œçš„**æ‰€æœ‰**å¯¦é«”å’Œé—œä¿‚ã€‚

ã€æ ¸å¿ƒä»»å‹™ã€‘
1. æå–ç›¡å¯èƒ½å¤šçš„å¯¦é«”ï¼ˆç›®æ¨™ï¼š15-30 å€‹ï¼‰
2. ç‚ºæ¯å€‹å¯¦é«”æä¾›è©³ç´°æè¿°
3. è­˜åˆ¥å¯¦é«”é–“çš„å„ç¨®é—œä¿‚
4. ä¸è¦éºæ¼ä»»ä½•é‡è¦è³‡è¨Š

ã€å¯¦é«”é¡å‹ã€‘ï¼ˆè«‹ç›¡é‡æ¶µè“‹ï¼‰
- å…¬å¸/çµ„ç¹”ï¼šç›¸é—œå…¬å¸ã€å­å…¬å¸ã€éƒ¨é–€ã€æ©Ÿæ§‹
- äººç‰©ï¼šå‰µå§‹äººã€é«˜ç®¡ã€è‘£äº‹ã€é‡è¦å“¡å·¥
- ç”¢å“/æœå‹™ï¼šä¸»è¦ç”¢å“ã€æœå‹™ã€å¹³å°
- æŠ€è¡“ï¼šä½¿ç”¨çš„æŠ€è¡“ã€æŠ€è¡“æ£§ã€ç®—æ³•ã€å·¥å…·
- ç«¶çˆ­å°æ‰‹ï¼šç›´æ¥ç«¶çˆ­è€…ã€é–“æ¥ç«¶çˆ­è€…ã€æ½›åœ¨å¨è„…
- åˆä½œå¤¥ä¼´ï¼šæˆ°ç•¥åˆä½œã€ä¾›æ‡‰å•†ã€å®¢æˆ¶
- æŠ•è³‡è€…ï¼šæŠ•è³‡æ©Ÿæ§‹ã€å‰µæŠ•ã€å¤©ä½¿æŠ•è³‡äºº
- äº‹ä»¶ï¼šèè³‡ã€æ”¶è³¼ã€ç™¼å¸ƒã€é‡Œç¨‹ç¢‘
- æŒ‡æ¨™ï¼šç‡Ÿæ”¶ã€ç”¨æˆ¶æ•¸ã€å¸‚å€¼ã€å¢é•·æ•¸æ“š
- åœ°é»ï¼šç¸½éƒ¨ã€è¾¦å…¬å®¤ã€å¸‚å ´å€åŸŸ
- æ¦‚å¿µï¼šç­–ç•¥ã€é¡˜æ™¯ã€å•†æ¥­æ¨¡å¼

ã€é—œä¿‚é¡å‹ã€‘ï¼ˆè«‹ç›¡é‡è­˜åˆ¥ï¼‰
å‰µç«‹ã€é ˜å°ã€æŠ•è³‡ã€æ”¶è³¼ã€åˆä½œã€ç«¶çˆ­ã€ä½¿ç”¨ã€é–‹ç™¼ã€ç™¼å¸ƒã€ä½æ–¼ã€æœå‹™æ–¼ç­‰

ã€æ–‡æª”è³‡è¨Šã€‘
æ¨™é¡Œï¼š{title}
å€å¡Šï¼š{chunk_idx}

ã€æ–‡æœ¬å…§å®¹ã€‘
{text[:3500]}

ã€è¼¸å‡ºæ ¼å¼ã€‘
å¿…é ˆè¿”å›æœ‰æ•ˆçš„ JSONï¼š
{{
  "entities": [
    {{
      "name": "å¯¦é«”åç¨±",
      "type": "å¯¦é«”é¡å‹ï¼ˆå¾ä¸Šè¿°é¡å‹ä¸­é¸æ“‡ï¼‰",
      "description": "è©³ç´°æè¿°ï¼ˆ50-100å­—ï¼‰ï¼Œèªªæ˜è©²å¯¦é«”çš„èƒŒæ™¯ã€ä½œç”¨ã€èˆ‡ä¸»é¡Œçš„é—œè¯",
      "importance": "high/medium/lowï¼ˆé‡è¦æ€§è©•ä¼°ï¼‰"
    }}
  ],
  "relationships": [
    {{
      "source": "æºå¯¦é«”åç¨±",
      "target": "ç›®æ¨™å¯¦é«”åç¨±",
      "relation": "é—œä¿‚é¡å‹",
      "description": "é—œä¿‚çš„è©³ç´°æè¿°",
      "strength": "strong/medium/weakï¼ˆé—œä¿‚å¼·åº¦ï¼‰"
    }}
  ]
}}

ã€ç¯„ä¾‹ã€‘
å¦‚æœä¸»é¡Œæ˜¯ "Tesla"ï¼š
{{
  "entities": [
    {{"name": "Tesla", "type": "å…¬å¸/çµ„ç¹”", "description": "ç¾åœ‹é›»å‹•è»Šè£½é€ å•†ï¼Œç”± Elon Musk é ˜å°ï¼Œå°ˆæ³¨æ–¼é›»å‹•è»Šå’Œæ¸…æ½”èƒ½æº", "importance": "high"}},
    {{"name": "Elon Musk", "type": "äººç‰©", "description": "Tesla CEOï¼Œä¼æ¥­å®¶ï¼ŒåŒæ™‚é ˜å° SpaceX å’Œ Xï¼ˆå‰ Twitterï¼‰", "importance": "high"}},
    {{"name": "Model 3", "type": "ç”¢å“/æœå‹™", "description": "Tesla æš¢éŠ·é›»å‹•è»Šå‹ï¼Œé¢å‘å¤§çœ¾å¸‚å ´", "importance": "medium"}},
    {{"name": "BYD", "type": "ç«¶çˆ­å°æ‰‹", "description": "ä¸­åœ‹é›»å‹•è»Šè£½é€ å•†ï¼Œå…¨çƒéŠ·é‡é ˜å…ˆ", "importance": "medium"}},
    {{"name": "Gigafactory", "type": "åœ°é»", "description": "Tesla åœ¨å…¨çƒçš„è¶…ç´šå·¥å» ï¼Œç”¨æ–¼å¤§è¦æ¨¡ç”Ÿç”¢", "importance": "medium"}}
  ],
  "relationships": [
    {{"source": "Elon Musk", "target": "Tesla", "relation": "é ˜å°", "description": "æ“”ä»» CEO ä¸¦æ¨å‹•å…¬å¸æˆ°ç•¥", "strength": "strong"}},
    {{"source": "Tesla", "target": "BYD", "relation": "ç«¶çˆ­", "description": "åœ¨é›»å‹•è»Šå¸‚å ´ç›´æ¥ç«¶çˆ­", "strength": "strong"}},
    {{"source": "Tesla", "target": "Model 3", "relation": "é–‹ç™¼", "description": "Tesla é–‹ç™¼ä¸¦ç”Ÿç”¢ Model 3", "strength": "strong"}}
  ]
}}

ç¾åœ¨è«‹é–‹å§‹æå–ï¼Œè¨˜ä½è¦**å…¨é¢ä¸”è©³ç´°**ï¼Œä¸è¦éºæ¼ä»»ä½•ç›¸é—œå¯¦é«”ï¼š"""

        response = self._call_ollama(prompt, temperature=0.1)
        return self._parse_json_response(response, title, url)

    def _extract_relationships_deep(self, text: str, title: str, url: str, query: str, existing_entities: List[Dict]) -> List[Dict]:
        """æ·±åº¦é—œä¿‚æŒ–æ˜ï¼ˆå°ˆæ³¨æ–¼é—œä¿‚ï¼‰"""
        
        # æå–å·²æœ‰å¯¦é«”åç¨±
        entity_names = [e["name"] for e in existing_entities[:20]]  # æœ€å¤šä½¿ç”¨ 20 å€‹
        
        prompt = f"""ä½ æ˜¯é—œä¿‚æŒ–æ˜å°ˆå®¶ã€‚è«‹åˆ†ææ–‡æœ¬ï¼Œæ‰¾å‡ºä»¥ä¸‹å¯¦é«”ä¹‹é–“çš„**æ‰€æœ‰å¯èƒ½é—œä¿‚**ã€‚

ã€å·²çŸ¥å¯¦é«”ã€‘
{', '.join(entity_names)}

ã€æ–‡æœ¬å…§å®¹ã€‘
{text[:3000]}

ã€ä»»å‹™ã€‘
1. æ‰¾å‡ºå·²çŸ¥å¯¦é«”ä¹‹é–“çš„é—œä¿‚
2. æ‰¾å‡ºå·²çŸ¥å¯¦é«”èˆ‡æ–‡æœ¬ä¸­å…¶ä»–å¯¦é«”çš„é—œä¿‚
3. è­˜åˆ¥éš±å«çš„ã€é–“æ¥çš„é—œä¿‚
4. æ¯å€‹é—œä¿‚éƒ½è¦æœ‰è©³ç´°æè¿°

ã€é—œä¿‚é¡å‹ã€‘
å‰µç«‹ã€é ˜å°ã€æŠ•è³‡ã€æ”¶è³¼ã€åˆä½œã€ç«¶çˆ­ã€ä½¿ç”¨ã€é–‹ç™¼ã€ç™¼å¸ƒã€æ”¯æŒã€å½±éŸ¿ã€åŸºæ–¼ã€å„ªæ–¼ã€æœå‹™æ–¼ç­‰

ã€è¼¸å‡ºæ ¼å¼ã€‘
{{
  "relationships": [
    {{
      "source": "å¯¦é«”A",
      "target": "å¯¦é«”B",
      "relation": "é—œä¿‚é¡å‹",
      "description": "è©³ç´°æè¿°é€™å€‹é—œä¿‚ï¼ŒåŒ…æ‹¬æ™‚é–“ã€æ–¹å¼ã€å½±éŸ¿ç­‰",
      "strength": "strong/medium/weak",
      "evidence": "æ–‡æœ¬ä¸­æ”¯æŒé€™å€‹é—œä¿‚çš„å…·é«”è­‰æ“š"
    }}
  ]
}}

è«‹ç›¡å¯èƒ½å¤šåœ°æå–é—œä¿‚ï¼ˆç›®æ¨™ï¼š10-20 å€‹é—œä¿‚ï¼‰ï¼š"""

        response = self._call_ollama(prompt, temperature=0.1)
        parsed = self._parse_json_response(response, title, url)
        return parsed.get("relationships", []) if parsed else []

    def _enhance_entity_context(self, entities: List[Dict], full_text: str, title: str, url: str) -> List[Dict]:
        """å¢å¼·å¯¦é«”ä¸Šä¸‹æ–‡ï¼ˆç‚ºé‡è¦å¯¦é«”æ·»åŠ æ›´å¤šè³‡è¨Šï¼‰"""
        
        # æŒ‘é¸æœ€é‡è¦çš„å¯¦é«”é€²è¡Œå¢å¼·
        important_entities = [e for e in entities if e.get("importance") == "high"][:10]
        
        if not important_entities:
            return entities
        
        entity_names = [e["name"] for e in important_entities]
        
        prompt = f"""è«‹ç‚ºä»¥ä¸‹å¯¦é«”æä¾›æ›´è±å¯Œçš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚

ã€å¯¦é«”åˆ—è¡¨ã€‘
{', '.join(entity_names)}

ã€æ–‡æª”ã€‘
{title}
{full_text[:4000]}

ã€ä»»å‹™ã€‘
ç‚ºæ¯å€‹å¯¦é«”æä¾›ï¼š
1. è©³ç´°çš„èƒŒæ™¯è³‡è¨Š
2. åœ¨æ–‡æª”ä¸­çš„è§’è‰²
3. èˆ‡ä¸»é¡Œçš„é—œè¯
4. ç›¸é—œçš„æ•¸æ“šæˆ–äº‹å¯¦

ã€è¼¸å‡ºæ ¼å¼ã€‘
{{
  "enhanced_entities": [
    {{
      "name": "å¯¦é«”åç¨±",
      "extended_description": "è±å¯Œçš„æè¿°ï¼ˆ100-200å­—ï¼‰",
      "key_facts": ["äº‹å¯¦1", "äº‹å¯¦2", "äº‹å¯¦3"],
      "mentions": 3
    }}
  ]
}}

è«‹æä¾›è©³ç´°è³‡è¨Šï¼š"""

        response = self._call_ollama(prompt, temperature=0.2)
        parsed = self._parse_json_response(response, title, url)
        
        if parsed and "enhanced_entities" in parsed:
            # åˆä½µå¢å¼·è³‡è¨Š
            enhanced_map = {e["name"]: e for e in parsed["enhanced_entities"]}
            
            for entity in entities:
                if entity["name"] in enhanced_map:
                    enhanced = enhanced_map[entity["name"]]
                    entity["description"] = enhanced.get("extended_description", entity.get("description", ""))
                    entity["key_facts"] = enhanced.get("key_facts", [])
                    entity["mentions"] = enhanced.get("mentions", 1)
        
        return entities

    # =========================
    # å¯¦é«”æ“´å±•èˆ‡æ¨æ–·
    # =========================

    def _expand_entities(self, unique_entities: List[Dict], all_entities: List[Dict], query: str) -> List[Dict]:
        """åŸºæ–¼å·²æœ‰å¯¦é«”ï¼ŒæŒ–æ˜æ›´å¤šé—œè¯å¯¦é«”"""
        
        # é¸æ“‡æœ€é‡è¦çš„å¯¦é«”ä½œç‚ºç¨®å­
        seed_entities = [e for e in unique_entities if e.get("importance") == "high"][:5]
        
        if not seed_entities:
            return []
        
        seed_names = [e["name"] for e in seed_entities]
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹æ ¸å¿ƒå¯¦é«”ï¼Œè«‹æ¨æ–·å’Œåˆ—å‡º**ç›¸é—œä½†å°šæœªæåŠ**çš„é‡è¦å¯¦é«”ã€‚

ã€æ ¸å¿ƒå¯¦é«”ã€‘
{', '.join(seed_names)}

ã€ä¸»é¡Œã€‘
{query}

ã€ä»»å‹™ã€‘
æ¨æ–·å¯èƒ½ç›¸é—œä½†æ–‡æª”ä¸­æœªæ˜ç¢ºæåˆ°çš„å¯¦é«”ï¼Œä¾‹å¦‚ï¼š
- ç›¸é—œçš„ç«¶çˆ­å°æ‰‹
- é—œéµçš„åˆä½œå¤¥ä¼´
- é‡è¦çš„æŠ€è¡“æˆ–å·¥å…·
- æ½›åœ¨çš„æŠ•è³‡è€…
- ç›¸é—œçš„å¸‚å ´æˆ–è¡Œæ¥­

ã€è¼¸å‡ºæ ¼å¼ã€‘
{{
  "inferred_entities": [
    {{
      "name": "æ¨æ–·çš„å¯¦é«”åç¨±",
      "type": "å¯¦é«”é¡å‹",
      "description": "ç‚ºä»€éº¼é€™å€‹å¯¦é«”å¯èƒ½ç›¸é—œ",
      "confidence": "high/medium/low",
      "reasoning": "æ¨æ–·ä¾æ“š"
    }}
  ]
}}

è«‹åˆ—å‡º 5-10 å€‹å¯èƒ½ç›¸é—œçš„å¯¦é«”ï¼š"""

        response = self._call_ollama(prompt, temperature=0.3)
        parsed = self._parse_json_response(response, "", "")
        
        if parsed and "inferred_entities" in parsed:
            inferred = parsed["inferred_entities"]
            # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„æ¨æ–·å¯¦é«”
            return [e for e in inferred if e.get("confidence") in ["high", "medium"]]
        
        return []

    def _infer_relationships(self, entities: List[Dict], relationships: List[Dict]) -> List[Dict]:
        """åŸºæ–¼å·²æœ‰å¯¦é«”å’Œé—œä¿‚ï¼Œæ¨æ–·éš±å«é—œä¿‚"""
        
        if len(entities) < 5 or len(relationships) < 3:
            return []
        
        entity_names = [e["name"] for e in entities[:15]]
        existing_rels = [(r["source"], r["target"], r["relation"]) for r in relationships[:10]]
        
        prompt = f"""åŸºæ–¼ä»¥ä¸‹å¯¦é«”å’Œå·²çŸ¥é—œä¿‚ï¼Œè«‹æ¨æ–·å¯èƒ½å­˜åœ¨çš„**éš±å«é—œä¿‚**ã€‚

ã€å¯¦é«”ã€‘
{', '.join(entity_names)}

ã€å·²çŸ¥é—œä¿‚ã€‘
{json.dumps(existing_rels[:10], ensure_ascii=False)}

ã€ä»»å‹™ã€‘
æ¨æ–·é‚è¼¯ä¸Šåˆç†ä½†æœªæ˜ç¢ºæåŠçš„é—œä¿‚ï¼Œä¾‹å¦‚ï¼š
- å¦‚æœ A é ˜å° Bï¼ŒB é–‹ç™¼ Cï¼Œå‰‡ A å¯èƒ½å½±éŸ¿ C
- å¦‚æœ X æŠ•è³‡ Yï¼ŒY ç«¶çˆ­ Zï¼Œå‰‡ X å¯èƒ½é—œæ³¨ Z
- å‚³éæ€§é—œä¿‚ã€éš±å«çš„åˆä½œæˆ–ç«¶çˆ­é—œä¿‚

ã€è¼¸å‡ºæ ¼å¼ã€‘
{{
  "inferred_relationships": [
    {{
      "source": "å¯¦é«”A",
      "target": "å¯¦é«”B",
      "relation": "æ¨æ–·çš„é—œä¿‚é¡å‹",
      "description": "æ¨æ–·ä¾æ“šå’Œé‚è¼¯",
      "confidence": "high/medium/low",
      "inferred": true
    }}
  ]
}}

è«‹åˆ—å‡º 3-8 å€‹åˆç†çš„æ¨æ–·é—œä¿‚ï¼š"""

        response = self._call_ollama(prompt, temperature=0.3)
        parsed = self._parse_json_response(response, "", "")
        
        if parsed and "inferred_relationships" in parsed:
            inferred = parsed["inferred_relationships"]
            # åªä¿ç•™ä¸­é«˜ç½®ä¿¡åº¦çš„æ¨æ–·é—œä¿‚
            return [r for r in inferred if r.get("confidence") in ["high", "medium"]]
        
        return []

    # =========================
    # LLM èª¿ç”¨
    # =========================

    def _call_ollama(self, prompt: str, temperature: float = 0.1) -> str:
        """èª¿ç”¨ Ollamaï¼ˆé‡å° GPU å„ªåŒ–ï¼‰"""
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "format": "json",
            "options": {
                "temperature": temperature,
                "num_predict": 3000,  # GPU æ”¯æŒæ›´é•·è¼¸å‡º
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1
            }
        }

        try:
            response = requests.post(
                f"{self.ollama_endpoint}/api/generate",
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json().get("response", "")
        except Exception as e:
            logger.error(f"âŒ Ollama èª¿ç”¨å¤±æ•—: {e}")
            return None

    def _parse_json_response(self, text: str, source_title: str, source_url: str) -> Dict[str, Any]:
        """è§£æ JSON å›æ‡‰"""
        if not text:
            return None
        
        try:
            # æ¸…ç† Markdown æ¨™è¨˜
            text = re.sub(r'```(json)?\s*', '', text)
            
            # æå– JSON
            match = re.search(r'\{.*\}', text, re.DOTALL)
            json_str = match.group(0) if match else text
            
            parsed = json.loads(json_str)
            
            # è£œå……ä¾†æºè³‡è¨Š
            for entity in parsed.get("entities", []):
                entity.setdefault("source_title", source_title)
                entity.setdefault("source_url", source_url)
                entity.setdefault("type", "æœªåˆ†é¡")
                entity.setdefault("description", "")
                entity.setdefault("importance", "medium")
            
            return parsed
            
        except Exception as e:
            logger.warning(f"âš ï¸ JSON è§£æå¤±æ•—: {e}")
            return None

    # =========================
    # é«˜ç´šå»é‡èˆ‡æ’åº
    # =========================

    def _advanced_deduplicate_entities(self, entities: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """é«˜ç´šå»é‡ï¼ˆè€ƒæ…®ç›¸ä¼¼åç¨±ã€åˆ¥åï¼‰"""
        if not entities:
            return []
        
        # æ§‹å»ºå¯¦é«”æ˜ å°„
        entity_map = {}
        
        for e in entities:
            name = e.get("name", "").strip()
            if not name or len(name) < 2:
                continue
            
            # æ¨™æº–åŒ–åç¨±
            normalized = self._normalize_entity_name(name)
            
            if normalized in entity_map:
                # åˆä½µå¯¦é«”è³‡è¨Šï¼ˆä¿ç•™æ›´è©³ç´°çš„ï¼‰
                existing = entity_map[normalized]
                
                # é¸æ“‡æ›´å¥½çš„æè¿°
                if len(e.get("description", "")) > len(existing.get("description", "")):
                    existing["description"] = e["description"]
                
                # åˆä½µä¾†æº
                if "sources" not in existing:
                    existing["sources"] = []
                existing["sources"].append({
                    "title": e.get("source_title", ""),
                    "url": e.get("source_url", "")
                })
                
                # åˆä½µé—œéµäº‹å¯¦
                if "key_facts" in e:
                    if "key_facts" not in existing:
                        existing["key_facts"] = []
                    existing["key_facts"].extend(e["key_facts"])
                
                # æå‡é‡è¦æ€§
                if e.get("importance") == "high":
                    existing["importance"] = "high"
                
            else:
                # æ–°å¯¦é«”
                e["normalized_name"] = normalized
                e["sources"] = [{
                    "title": e.get("source_title", ""),
                    "url": e.get("source_url", "")
                }]
                entity_map[normalized] = e
        
        return list(entity_map.values())

    def _normalize_entity_name(self, name: str) -> str:
        """æ¨™æº–åŒ–å¯¦é«”åç¨±"""
        # ç§»é™¤æ¨™é»ã€ç©ºæ ¼ï¼Œè½‰å°å¯«
        normalized = re.sub(r'[^\w\s]', '', name.lower())
        normalized = re.sub(r'\s+', '', normalized)
        
        # ç§»é™¤å¸¸è¦‹å¾Œç¶´
        suffixes = ['inc', 'ltd', 'llc', 'corp', 'corporation', 'company', 'co']
        for suffix in suffixes:
            if normalized.endswith(suffix):
                normalized = normalized[:-len(suffix)]
        
        return normalized.strip()

    def _advanced_deduplicate_relationships(self, relationships: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é«˜ç´šå»é‡é—œä¿‚"""
        seen = set()
        unique = []
        
        for r in relationships:
            source = self._normalize_entity_name(r.get("source", ""))
            target = self._normalize_entity_name(r.get("target", ""))
            relation = r.get("relation", "").strip().lower()
            
            # å‰µå»ºå”¯ä¸€éµï¼ˆè€ƒæ…®é›™å‘é—œä¿‚ï¼‰
            key1 = (source, relation, target)
            key2 = (target, self._reverse_relation(relation), source)
            
            if key1 not in seen and key2 not in seen and source and target:
                seen.add(key1)
                unique.append(r)
        
        return unique

    def _reverse_relation(self, relation: str) -> str:
        """ç²å–åå‘é—œä¿‚"""
        reverse_map = {
            "é ˜å°": "è¢«é ˜å°",
            "æŠ•è³‡": "è¢«æŠ•è³‡",
            "æ”¶è³¼": "è¢«æ”¶è³¼",
            "å‰µç«‹": "ç”±å‰µç«‹",
            "ä½¿ç”¨": "è¢«ä½¿ç”¨",
            "é–‹ç™¼": "è¢«é–‹ç™¼"
        }
        return reverse_map.get(relation, relation)

    def _score_and_rank_entities(self, entities: List[Dict], relationships: List[Dict], query: str) -> List[Dict]:
        """ç‚ºå¯¦é«”è©•åˆ†ä¸¦æ’åº"""
        
        # è¨ˆç®—æ¯å€‹å¯¦é«”åœ¨é—œä¿‚ä¸­å‡ºç¾çš„æ¬¡æ•¸
        entity_mentions = {}
        for r in relationships:
            source = r.get("source", "")
            target = r.get("target", "")
            entity_mentions[source] = entity_mentions.get(source, 0) + 1
            entity_mentions[target] = entity_mentions.get(target, 0) + 1
        
        q_lower = query.lower()
        
        for entity in entities:
            name = entity.get("name", "")
            name_lower = name.lower()
            desc = entity.get("description", "").lower()
            
            score = 0
            
            # 1. åç¨±åŒ…å«æŸ¥è©¢è©ï¼ˆé«˜æ¬Šé‡ï¼‰
            if q_lower in name_lower:
                score += 10
            
            # 2. æè¿°åŒ…å«æŸ¥è©¢è©
            if q_lower in desc:
                score += 5
            
            # 3. é‡è¦æ€§è©•ä¼°
            importance = entity.get("importance", "medium")
            if importance == "high":
                score += 8
            elif importance == "medium":
                score += 4
            
            # 4. é—œä¿‚è±å¯Œåº¦ï¼ˆåœ¨é—œä¿‚ç¶²ä¸­çš„ä¸­å¿ƒæ€§ï¼‰
            mention_count = entity_mentions.get(name, 0)
            score += min(mention_count * 2, 10)
            
            # 5. æè¿°è±å¯Œåº¦
            desc_length = len(entity.get("description", ""))
            if desc_length > 100:
                score += 3
            elif desc_length > 50:
                score += 1
            
            # 6. æœ‰é—œéµäº‹å¯¦
            if entity.get("key_facts"):
                score += len(entity["key_facts"])
            
            # 7. å¤šä¾†æºé©—è­‰
            if entity.get("sources"):
                score += min(len(entity["sources"]), 5)
            
            entity["relevance_score"] = score
        
        # æŒ‰è©•åˆ†æ’åº
        entities.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return entities

    # =========================
    # æ‘˜è¦ç”Ÿæˆ
    # =========================

    def _generate_document_summary(self, entities: List[Dict], relationships: List[Dict], title: str, query: str) -> str:
        """ç”Ÿæˆå–®å€‹æ–‡æª”çš„æ‘˜è¦"""
        if not entities:
            return f"{title} - æœªèƒ½æå–è³‡è¨Š"
        
        entity_types = {}
        for e in entities:
            etype = e.get("type", "æœªåˆ†é¡")
            entity_types[etype] = entity_types.get(etype, 0) + 1
        
        type_summary = ", ".join([f"{k}({v}å€‹)" for k, v in sorted(entity_types.items(), key=lambda x: -x[1])[:3]])
        
        return f"{title} - æå–äº† {len(entities)} å€‹å¯¦é«”å’Œ {len(relationships)} å€‹é—œä¿‚ï¼Œä¸»è¦åŒ…æ‹¬ï¼š{type_summary}"

    def _generate_comprehensive_summary(self, entities: List[Dict], relationships: List[Dict], 
                                       doc_summaries: List[Dict], query: str) -> str:
        """ç”Ÿæˆå…¨é¢çš„æ•´é«”æ‘˜è¦"""
        
        prompt = f"""è«‹åŸºæ–¼ä»¥ä¸‹è³‡è¨Šï¼Œç”Ÿæˆä¸€ä»½é—œæ–¼ã€Œ{query}ã€çš„å…¨é¢åˆ†ææ‘˜è¦ï¼ˆ200-300å­—ï¼‰ã€‚

ã€æå–çš„å¯¦é«”æ•¸é‡ã€‘
ç¸½è¨ˆï¼š{len(entities)} å€‹å¯¦é«”

ã€ä¸»è¦å¯¦é«”é¡å‹åˆ†å¸ƒã€‘
{self._get_entity_type_distribution(entities)}

ã€é—œä¿‚æ•¸é‡ã€‘
ç¸½è¨ˆï¼š{len(relationships)} å€‹é—œä¿‚

ã€å‰ 10 å€‹æœ€é‡è¦å¯¦é«”ã€‘
{self._get_top_entities_summary(entities[:10])}

ã€æ–‡æª”ä¾†æºã€‘
è™•ç†äº† {len(doc_summaries)} å€‹ä¾†æº

ã€ä»»å‹™ã€‘
ç”Ÿæˆä¸€ä»½ç°¡æ½”çš„æ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š
1. {query} çš„æ ¸å¿ƒå®šä½å’Œæ¥­å‹™
2. é—œéµäººç‰©å’Œçµ„ç¹”æ¶æ§‹
3. ä¸»è¦ç”¢å“æˆ–æœå‹™
4. é‡è¦çš„åˆä½œæˆ–ç«¶çˆ­é—œä¿‚
5. å€¼å¾—é—œæ³¨çš„äº‹ä»¶æˆ–æ•¸æ“š

è«‹ç”¨æµæš¢çš„ä¸­æ–‡æ’°å¯«ï¼š"""

        response = self._call_ollama(prompt, temperature=0.2)
        
        if response:
            # å˜—è©¦æå–æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯ JSON æˆ–ç´”æ–‡æœ¬ï¼‰
            try:
                parsed = json.loads(response)
                return parsed.get("summary", response)
            except:
                # ç›´æ¥è¿”å›æ–‡æœ¬
                return response.strip()
        
        return f"é—œæ–¼ {query} çš„è³‡è¨Šå·²å¾ {len(doc_summaries)} å€‹ä¾†æºæå–å®Œæˆï¼ŒåŒ…å« {len(entities)} å€‹å¯¦é«”å’Œ {len(relationships)} å€‹é—œä¿‚ã€‚"

    def _get_entity_type_distribution(self, entities: List[Dict]) -> str:
        """ç²å–å¯¦é«”é¡å‹åˆ†å¸ƒ"""
        type_count = {}
        for e in entities:
            etype = e.get("type", "æœªåˆ†é¡")
            type_count[etype] = type_count.get(etype, 0) + 1
        
        return ", ".join([f"{k}: {v}" for k, v in sorted(type_count.items(), key=lambda x: -x[1])[:5]])

    def _get_top_entities_summary(self, entities: List[Dict]) -> str:
        """ç²å–é ‚ç´šå¯¦é«”æ‘˜è¦"""
        summaries = []
        for e in entities[:10]:
            name = e.get("name", "")
            etype = e.get("type", "")
            summaries.append(f"- {name} ({etype})")
        return "\n".join(summaries)

    # =========================
    # çµ±è¨ˆè¼”åŠ©
    # =========================

    def _count_entity_types(self, entities: List[Dict]) -> Dict[str, int]:
        """çµ±è¨ˆå¯¦é«”é¡å‹"""
        counts = {}
        for e in entities:
            etype = e.get("type", "æœªåˆ†é¡")
            counts[etype] = counts.get(etype, 0) + 1
        return counts

    def _count_relationship_types(self, relationships: List[Dict]) -> Dict[str, int]:
        """çµ±è¨ˆé—œä¿‚é¡å‹"""
        counts = {}
        for r in relationships:
            rtype = r.get("relation", "æœªåˆ†é¡")
            counts[rtype] = counts.get(rtype, 0) + 1
        return counts